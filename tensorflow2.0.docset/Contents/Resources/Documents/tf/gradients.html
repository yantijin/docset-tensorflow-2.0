
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href=../../default.css" rel="stylesheet">
    <link href="
   ../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.gradients" />
<meta itemprop="path" content="Stable" />
</div>

<h1 id="tfgradients">tf.gradients</h1>
<!-- Insert buttons -->

<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p><a target="_blank" href="/code/stable/tensorflow/python/ops/gradients_impl.py">View source</a></p>
<!-- Start diff -->

<p>Constructs symbolic derivatives of sum of <code>ys</code> w.r.t. x in <code>xs</code>.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v2.gradients</code></li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span>
    <span class="n">ys</span><span class="p">,</span>
    <span class="n">xs</span><span class="p">,</span>
    <span class="n">grad_ys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;gradients&#39;</span><span class="p">,</span>
    <span class="n">gate_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">aggregation_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">stop_gradients</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">unconnected_gradients</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">UnconnectedGradients</span><span class="o">.</span><span class="n">NONE</span>
<span class="p">)</span>
</pre></div>


<!-- Placeholder for "Used in" -->

<p><code>ys</code> and <code>xs</code> are each a <code>Tensor</code> or a list of tensors.  <code>grad_ys</code>
is a list of <code>Tensor</code>, holding the gradients received by the
<code>ys</code>. The list must be the same length as <code>ys</code>.</p>
<p><code>gradients()</code> adds ops to the graph to output the derivatives of <code>ys</code> with
respect to <code>xs</code>.  It returns a list of <code>Tensor</code> of length <code>len(xs)</code> where
each tensor is the <code>sum(dy/dx)</code> for y in <code>ys</code>.</p>
<p><code>grad_ys</code> is a list of tensors of the same length as <code>ys</code> that holds
the initial gradients for each y in <code>ys</code>.  When <code>grad_ys</code> is None,
we fill in a tensor of '1's of the shape of y for each y in <code>ys</code>.  A
user can provide their own initial <code>grad_ys</code> to compute the
derivatives using a different initial gradient for each y (e.g., if
one wanted to weight the gradient differently for each value in
each y).</p>
<p><code>stop_gradients</code> is a <code>Tensor</code> or a list of tensors to be considered constant
with respect to all <code>xs</code>. These tensors will not be backpropagated through,
as though they had been explicitly disconnected using <code>stop_gradient</code>.  Among
other things, this allows computation of partial derivatives as opposed to
total derivatives. For example:</p>
<div class="codehilite"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">a</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">stop_gradients</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
</pre></div>


<p>Here the partial derivatives <code>g</code> evaluate to <code>[1.0, 1.0]</code>, compared to the
total derivatives <code>tf.gradients(a + b, [a, b])</code>, which take into account the
influence of <code>a</code> on <code>b</code> and evaluate to <code>[3.0, 1.0]</code>.  Note that the above is
equivalent to:</p>
<div class="codehilite"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">a</span><span class="p">)</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span> <span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
</pre></div>


<p><code>stop_gradients</code> provides a way of stopping gradient after the graph has
already been constructed, as compared to <a href="../tf/stop_gradient.html"><code>tf.stop_gradient</code></a> which is used
during graph construction.  When the two approaches are combined,
backpropagation stops at both <a href="../tf/stop_gradient.html"><code>tf.stop_gradient</code></a> nodes and nodes in
<code>stop_gradients</code>, whichever is encountered first.</p>
<p>All integer tensors are considered constant with respect to all <code>xs</code>, as if
they were included in <code>stop_gradients</code>.</p>
<p><code>unconnected_gradients</code> determines the value returned for each x in xs if it
is unconnected in the graph to ys. By default this is None to safeguard
against errors. Mathematically these gradients are zero which can be requested
using the <code>'zero'</code> option. <code>tf.UnconnectedGradients</code> provides the
following options and behaviors:</p>
<div class="codehilite"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">g1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">([</span><span class="n">b</span><span class="p">],</span> <span class="p">[</span><span class="n">a</span><span class="p">],</span> <span class="n">unnconnected_gradients</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">g1</span><span class="p">)</span>  <span class="c1"># [None]</span>

<span class="n">g2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">([</span><span class="n">b</span><span class="p">],</span> <span class="p">[</span><span class="n">a</span><span class="p">],</span> <span class="n">unconnected_gradients</span><span class="o">=</span><span class="s1">&#39;zero&#39;</span><span class="p">)</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">g2</span><span class="p">)</span>  <span class="c1"># [array([[0., 0.]], dtype=float32)]</span>
</pre></div>


<h4 id="args">Args:</h4>
<ul>
<li><b><code>ys</code></b>: A <code>Tensor</code> or list of tensors to be differentiated.</li>
<li><b><code>xs</code></b>: A <code>Tensor</code> or list of tensors to be used for differentiation.</li>
<li><b><code>grad_ys</code></b>: Optional. A <code>Tensor</code> or list of tensors the same size as
  <code>ys</code> and holding the gradients computed for each y in <code>ys</code>.</li>
<li><b><code>name</code></b>: Optional name to use for grouping all the gradient ops together.
  defaults to 'gradients'.</li>
<li><b><code>gate_gradients</code></b>: If True, add a tuple around the gradients returned
  for an operations.  This avoids some race conditions.</li>
<li><b><code>aggregation_method</code></b>: Specifies the method used to combine gradient terms.
  Accepted values are constants defined in the class <code>AggregationMethod</code>.</li>
<li><b><code>stop_gradients</code></b>: Optional. A <code>Tensor</code> or list of tensors not to differentiate
  through.</li>
<li><b><code>unconnected_gradients</code></b>: Optional. Specifies the gradient value returned when
  the given input tensors are unconnected. Accepted values are constants
  defined in the class <a href="../tf/UnconnectedGradients.html"><code>tf.UnconnectedGradients</code></a> and the default value is
  <code>none</code>.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A list of <code>sum(dy/dx)</code> for each x in <code>xs</code>.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>LookupError</code></b>: if one of the operations between <code>x</code> and <code>y</code> does not
  have a registered gradient function.</li>
<li><b><code>ValueError</code></b>: if the arguments are invalid.</li>
<li><b><code>RuntimeError</code></b>: if called in Eager mode.</li>
</ul>
    </body>
    </html>
   