<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.random.gamma" />
<meta itemprop="path" content="Stable" />
</div>


<h1>tf.random.gamma</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/ops/random_ops.py">View source</a></p>

<!-- Start diff -->


<p>Draws <code>shape</code> samples from each of the given Gamma distribution(s).</p>

<h3>Aliases:</h3>

<ul>
<li><code>tf.compat.v1.random.gamma</code></li>
<li><code>tf.compat.v1.random_gamma</code></li>
<li><code>tf.compat.v2.random.gamma</code></li>
</ul>


<p><code>python
tf.random.gamma(
    shape,
    alpha,
    beta=None,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
</code></p>

<!-- Placeholder for "Used in" -->


<p><code>alpha</code> is the shape parameter describing the distribution(s), and <code>beta</code> is
the inverse scale parameter(s).</p>

<p>Note: Because internal calculations are done using <code>float64</code> and casting has
<code>floor</code> semantics, we must manually map zero outcomes to the smallest
possible positive floating-point value, i.e., <code>np.finfo(dtype).tiny</code>.  This
means that <code>np.finfo(dtype).tiny</code> occurs more frequently than it otherwise
should.  This bias can only happen for small values of <code>alpha</code>, i.e.,
<code>alpha &lt;&lt; 1</code> or large values of <code>beta</code>, i.e., <code>beta &gt;&gt; 1</code>.</p>

<p>The samples are differentiable w.r.t. alpha and beta.
The derivatives are computed using the approach described in the paper</p>

<p><a href="https://arxiv.org/abs/1805.08498">Michael Figurnov, Shakir Mohamed, Andriy Mnih.
Implicit Reparameterization Gradients, 2018</a></p>

<h4>Example:</h4>

<p>```python
samples = tf.random.gamma([10], [0.5, 1.5])</p>

<h1>samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents</h1>

<h1>the samples drawn from each distribution</h1>

<p>samples = tf.random.gamma([7, 5], [0.5, 1.5])</p>

<h1>samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1]</h1>

<h1>represents the 7x5 samples drawn from each of the two distributions</h1>

<p>alpha = tf.constant([[1.],[3.],[5.]])
beta = tf.constant([[3., 4.]])
samples = tf.random.gamma([30], alpha=alpha, beta=beta)</p>

<h1>samples has shape [30, 3, 2], with 30 samples each of 3x2 distributions.</h1>

<p>loss = tf.reduce_mean(tf.square(samples))
dloss_dalpha, dloss_dbeta = tf.gradients(loss, [alpha, beta])</p>

<h1>unbiased stochastic derivatives of the loss function</h1>

<p>alpha.shape == dloss_dalpha.shape  # True
beta.shape == dloss_dbeta.shape  # True
```</p>

<h4>Args:</h4>

<ul>
<li><b><code>shape</code></b>: A 1-D integer Tensor or Python array. The shape of the output samples
to be drawn per alpha/beta-parameterized distribution.</li>
<li><b><code>alpha</code></b>: A Tensor or Python value or N-D array of type <code>dtype</code>. <code>alpha</code>
provides the shape parameter(s) describing the gamma distribution(s) to
sample. Must be broadcastable with <code>beta</code>.</li>
<li><b><code>beta</code></b>: A Tensor or Python value or N-D array of type <code>dtype</code>. Defaults to 1.
<code>beta</code> provides the inverse scale parameter(s) of the gamma
distribution(s) to sample. Must be broadcastable with <code>alpha</code>.</li>
<li><b><code>dtype</code></b>: The type of alpha, beta, and the output: <code>float16</code>, <code>float32</code>, or
<code>float64</code>.</li>
<li><b><code>seed</code></b>: A Python integer. Used to create a random seed for the distributions.
See
<a href="../../tf/compat/v1/set_random_seed.html"><code>tf.compat.v1.set_random_seed</code></a>
for behavior.</li>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>


<h4>Returns:</h4>

<ul>
<li><b><code>samples</code></b>: a <code>Tensor</code> of shape
<code>tf.concat([shape, tf.shape(alpha + beta)], axis=0)</code> with values of type
<code>dtype</code>.</li>
</ul>

