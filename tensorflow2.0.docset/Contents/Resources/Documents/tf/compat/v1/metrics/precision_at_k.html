<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.compat.v1.metrics.precision_at_k" />
<meta itemprop="path" content="Stable" />
</div>


<h1>tf.compat.v1.metrics.precision_at_k</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/ops/metrics_impl.py">View source</a></p>

<!-- Start diff -->


<p>Computes precision@k of the predictions with respect to sparse labels.</p>

<p><code>python
tf.compat.v1.metrics.precision_at_k(
    labels,
    predictions,
    k,
    class_id=None,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
</code></p>

<!-- Placeholder for "Used in" -->


<p>If <code>class_id</code> is specified, we calculate precision by considering only the
    entries in the batch for which <code>class_id</code> is in the top-k highest
    <code>predictions</code>, and computing the fraction of them for which <code>class_id</code> is
    indeed a correct label.
If <code>class_id</code> is not specified, we&rsquo;ll calculate precision as how often on
    average a class among the top-k classes with the highest predicted values
    of a batch entry is correct and can be found in the label for that entry.</p>

<p><code>precision_at_k</code> creates two local variables,
<code>true_positive_at_&lt;k&gt;</code> and <code>false_positive_at_&lt;k&gt;</code>, that are used to compute
the precision@k frequency. This frequency is ultimately returned as
<code>precision_at_&lt;k&gt;</code>: an idempotent operation that simply divides
<code>true_positive_at_&lt;k&gt;</code> by total (<code>true_positive_at_&lt;k&gt;</code> +
<code>false_positive_at_&lt;k&gt;</code>).</p>

<p>For estimation of the metric over a stream of data, the function creates an
<code>update_op</code> operation that updates these variables and returns the
<code>precision_at_&lt;k&gt;</code>. Internally, a <code>top_k</code> operation computes a <code>Tensor</code>
indicating the top <code>k</code> <code>predictions</code>. Set operations applied to <code>top_k</code> and
<code>labels</code> calculate the true positives and false positives weighted by
<code>weights</code>. Then <code>update_op</code> increments <code>true_positive_at_&lt;k&gt;</code> and
<code>false_positive_at_&lt;k&gt;</code> using these values.</p>

<p>If <code>weights</code> is <code>None</code>, weights default to 1. Use weights of 0 to mask values.</p>

<h4>Args:</h4>

<ul>
<li><b><code>labels</code></b>: <code>int64</code> <code>Tensor</code> or <code>SparseTensor</code> with shape
[D1, &hellip; DN, num_labels] or [D1, &hellip; DN], where the latter implies
num_labels=1. N >= 1 and num_labels is the number of target classes for
the associated prediction. Commonly, N=1 and <code>labels</code> has shape
[batch_size, num_labels]. [D1, &hellip; DN] must match <code>predictions</code>. Values
should be in range [0, num_classes), where num_classes is the last
dimension of <code>predictions</code>. Values outside this range are ignored.</li>
<li><b><code>predictions</code></b>: Float <code>Tensor</code> with shape [D1, &hellip; DN, num_classes] where
N >= 1. Commonly, N=1 and predictions has shape [batch size, num_classes].
The final dimension contains the logit values for each class. [D1, &hellip; DN]
must match <code>labels</code>.</li>
<li><b><code>k</code></b>: Integer, k for @k metric.</li>
<li><b><code>class_id</code></b>: Integer class ID for which we want binary metrics. This should be
in range [0, num_classes], where num_classes is the last dimension of
<code>predictions</code>. If <code>class_id</code> is outside this range, the method returns
NAN.</li>
<li><b><code>weights</code></b>: <code>Tensor</code> whose rank is either 0, or n-1, where n is the rank of
<code>labels</code>. If the latter, it must be broadcastable to <code>labels</code> (i.e., all
dimensions must be either <code>1</code>, or the same as the corresponding <code>labels</code>
dimension).</li>
<li><b><code>metrics_collections</code></b>: An optional list of collections that values should
be added to.</li>
<li><b><code>updates_collections</code></b>: An optional list of collections that updates should
be added to.</li>
<li><b><code>name</code></b>: Name of new update operation, and namespace for other dependent ops.</li>
</ul>


<h4>Returns:</h4>

<ul>
<li><b><code>precision</code></b>: Scalar <code>float64</code> <code>Tensor</code> with the value of <code>true_positives</code>
divided by the sum of <code>true_positives</code> and <code>false_positives</code>.</li>
<li><b><code>update_op</code></b>: <code>Operation</code> that increments <code>true_positives</code> and
<code>false_positives</code> variables appropriately, and whose value matches
<code>precision</code>.</li>
</ul>


<h4>Raises:</h4>

<ul>
<li><b><code>ValueError</code></b>: If <code>weights</code> is not <code>None</code> and its shape doesn&rsquo;t match
<code>predictions</code>, or if either <code>metrics_collections</code> or <code>updates_collections</code>
are not a list or tuple.</li>
<li><b><code>RuntimeError</code></b>: If eager execution is enabled.</li>
</ul>

