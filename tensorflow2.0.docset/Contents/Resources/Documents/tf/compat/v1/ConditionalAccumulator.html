<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.compat.v1.ConditionalAccumulator" />
<meta itemprop="path" content="Stable" />
<meta itemprop="property" content="accumulator_ref"/>
<meta itemprop="property" content="dtype"/>
<meta itemprop="property" content="name"/>
<meta itemprop="property" content="__init__"/>
<meta itemprop="property" content="apply_grad"/>
<meta itemprop="property" content="num_accumulated"/>
<meta itemprop="property" content="set_global_step"/>
<meta itemprop="property" content="take_grad"/>
</div>


<h1>tf.compat.v1.ConditionalAccumulator</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/ops/data_flow_ops.py">View source</a></p>

<h2>Class <code>ConditionalAccumulator</code></h2>

<!-- Start diff -->


<p>A conditional accumulator for aggregating gradients.</p>

<p>Inherits From: <a href="../../../tf/compat/v1/ConditionalAccumulatorBase.html"><code>ConditionalAccumulatorBase</code></a></p>

<!-- Placeholder for "Used in" -->


<p>Up-to-date gradients (i.e., time step at which gradient was computed is
equal to the accumulator&rsquo;s time step) are added to the accumulator.</p>

<p>Extraction of the average gradient is blocked until the required number of
gradients has been accumulated.</p>

<h2 id="__init__"><code>__init__</code></h2>


<p><a target="_blank" href="/code/stable/tensorflow/python/ops/data_flow_ops.py">View source</a></p>

<p><code>python
__init__(
    dtype,
    shape=None,
    shared_name=None,
    name='conditional_accumulator',
    reduction_type='MEAN'
)
</code></p>

<p>Creates a new ConditionalAccumulator.</p>

<h4>Args:</h4>

<ul>
<li><b><code>dtype</code></b>: Datatype of the accumulated gradients.</li>
<li><b><code>shape</code></b>: Shape of the accumulated gradients.</li>
<li><b><code>shared_name</code></b>: Optional. If non-empty, this accumulator will be shared under
the given name across multiple sessions.</li>
<li><b><code>name</code></b>: Optional name for the accumulator.</li>
<li><b><code>reduction_type</code></b>: Reduction type to use when taking the gradient.</li>
</ul>


<h2>Properties</h2>

<h3 id="accumulator_ref"><code>accumulator_ref</code></h3>


<p>The underlying accumulator reference.</p>

<h3 id="dtype"><code>dtype</code></h3>


<p>The datatype of the gradients accumulated by this accumulator.</p>

<h3 id="name"><code>name</code></h3>


<p>The name of the underlying accumulator.</p>

<h2>Methods</h2>

<h3 id="apply_grad"><code>apply_grad</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/ops/data_flow_ops.py">View source</a></p>

<p><code>python
apply_grad(
    grad,
    local_step=0,
    name=None
)
</code></p>

<p>Attempts to apply a gradient to the accumulator.</p>

<p>The attempt is silently dropped if the gradient is stale, i.e., local_step
is less than the accumulator&rsquo;s global time step.</p>

<h4>Args:</h4>

<ul>
<li><b><code>grad</code></b>: The gradient tensor to be applied.</li>
<li><b><code>local_step</code></b>: Time step at which the gradient was computed.</li>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>


<h4>Returns:</h4>

<p>The operation that (conditionally) applies a gradient to the accumulator.</p>

<h4>Raises:</h4>

<ul>
<li><b><code>ValueError</code></b>: If grad is of the wrong shape</li>
</ul>


<h3 id="num_accumulated"><code>num_accumulated</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/ops/data_flow_ops.py">View source</a></p>

<p><code>python
num_accumulated(name=None)
</code></p>

<p>Number of gradients that have currently been aggregated in accumulator.</p>

<h4>Args:</h4>

<ul>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>


<h4>Returns:</h4>

<p>Number of accumulated gradients currently in accumulator.</p>

<h3 id="set_global_step"><code>set_global_step</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/ops/data_flow_ops.py">View source</a></p>

<p><code>python
set_global_step(
    new_global_step,
    name=None
)
</code></p>

<p>Sets the global time step of the accumulator.</p>

<p>The operation logs a warning if we attempt to set to a time step that is
lower than the accumulator&rsquo;s own time step.</p>

<h4>Args:</h4>

<ul>
<li><b><code>new_global_step</code></b>: Value of new time step. Can be a variable or a constant</li>
<li><b><code>name</code></b>: Optional name for the operation.</li>
</ul>


<h4>Returns:</h4>

<p>Operation that sets the accumulator&rsquo;s time step.</p>

<h3 id="take_grad"><code>take_grad</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/ops/data_flow_ops.py">View source</a></p>

<p><code>python
take_grad(
    num_required,
    name=None
)
</code></p>

<p>Attempts to extract the average gradient from the accumulator.</p>

<p>The operation blocks until sufficient number of gradients have been
successfully applied to the accumulator.</p>

<p>Once successful, the following actions are also triggered:</p>

<ul>
<li>Counter of accumulated gradients is reset to 0.</li>
<li>Aggregated gradient is reset to 0 tensor.</li>
<li>Accumulator&rsquo;s internal time step is incremented by 1.</li>
</ul>


<h4>Args:</h4>

<ul>
<li><b><code>num_required</code></b>: Number of gradients that needs to have been aggregated</li>
<li><b><code>name</code></b>: Optional name for the operation</li>
</ul>


<h4>Returns:</h4>

<p>A tensor holding the value of the average gradient.</p>

<h4>Raises:</h4>

<ul>
<li><b><code>InvalidArgumentError</code></b>: If num_required &lt; 1</li>
</ul>

