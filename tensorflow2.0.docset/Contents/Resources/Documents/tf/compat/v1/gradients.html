<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.compat.v1.gradients" />
<meta itemprop="path" content="Stable" />
</div>


<h1>tf.compat.v1.gradients</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/ops/gradients_impl.py">View source</a></p>

<!-- Start diff -->


<p>Constructs symbolic derivatives of sum of <code>ys</code> w.r.t. x in <code>xs</code>.</p>

<p><code>python
tf.compat.v1.gradients(
    ys,
    xs,
    grad_ys=None,
    name='gradients',
    colocate_gradients_with_ops=False,
    gate_gradients=False,
    aggregation_method=None,
    stop_gradients=None,
    unconnected_gradients=tf.UnconnectedGradients.NONE
)
</code></p>

<!-- Placeholder for "Used in" -->


<p><code>ys</code> and <code>xs</code> are each a <code>Tensor</code> or a list of tensors.  <code>grad_ys</code>
is a list of <code>Tensor</code>, holding the gradients received by the
<code>ys</code>. The list must be the same length as <code>ys</code>.</p>

<p><code>gradients()</code> adds ops to the graph to output the derivatives of <code>ys</code> with
respect to <code>xs</code>.  It returns a list of <code>Tensor</code> of length <code>len(xs)</code> where
each tensor is the <code>sum(dy/dx)</code> for y in <code>ys</code>.</p>

<p><code>grad_ys</code> is a list of tensors of the same length as <code>ys</code> that holds
the initial gradients for each y in <code>ys</code>.  When <code>grad_ys</code> is None,
we fill in a tensor of &lsquo;1&rsquo;s of the shape of y for each y in <code>ys</code>.  A
user can provide their own initial <code>grad_ys</code> to compute the
derivatives using a different initial gradient for each y (e.g., if
one wanted to weight the gradient differently for each value in
each y).</p>

<p><code>stop_gradients</code> is a <code>Tensor</code> or a list of tensors to be considered constant
with respect to all <code>xs</code>. These tensors will not be backpropagated through,
as though they had been explicitly disconnected using <code>stop_gradient</code>.  Among
other things, this allows computation of partial derivatives as opposed to
total derivatives. For example:</p>

<p><code>python
a = tf.constant(0.)
b = 2 * a
g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])
</code></p>

<p>Here the partial derivatives <code>g</code> evaluate to <code>[1.0, 1.0]</code>, compared to the
total derivatives <code>tf.gradients(a + b, [a, b])</code>, which take into account the
influence of <code>a</code> on <code>b</code> and evaluate to <code>[3.0, 1.0]</code>.  Note that the above is
equivalent to:</p>

<p><code>python
a = tf.stop_gradient(tf.constant(0.))
b = tf.stop_gradient(2 * a)
g = tf.gradients(a + b, [a, b])
</code></p>

<p><code>stop_gradients</code> provides a way of stopping gradient after the graph has
already been constructed, as compared to <a href="../../../tf/stop_gradient.html"><code>tf.stop_gradient</code></a> which is used
during graph construction.  When the two approaches are combined,
backpropagation stops at both <a href="../../../tf/stop_gradient.html"><code>tf.stop_gradient</code></a> nodes and nodes in
<code>stop_gradients</code>, whichever is encountered first.</p>

<p>All integer tensors are considered constant with respect to all <code>xs</code>, as if
they were included in <code>stop_gradients</code>.</p>

<p><code>unconnected_gradients</code> determines the value returned for each x in xs if it
is unconnected in the graph to ys. By default this is None to safeguard
against errors. MAthematically these gradients are zero which can be requested
using the <code>'zero'</code> option. <code>tf.UnconnectedGradients</code> provides the
following options and behaviors:</p>

<p>```python
a = tf.ones([1, 2])
b = tf.ones([3, 1])
g1 = tf.gradients([b], [a], unnconnected_gradients=&lsquo;none&rsquo;)
sess.run(g1)  # [None]</p>

<p>g2 = tf.gradients([b], [a], unconnected_gradients=&lsquo;zero&rsquo;)
sess.run(g2)  # [array([[0., 0.]], dtype=float32)]
```</p>

<h4>Args:</h4>

<ul>
<li><b><code>ys</code></b>: A <code>Tensor</code> or list of tensors to be differentiated.</li>
<li><b><code>xs</code></b>: A <code>Tensor</code> or list of tensors to be used for differentiation.</li>
<li><b><code>grad_ys</code></b>: Optional. A <code>Tensor</code> or list of tensors the same size as
<code>ys</code> and holding the gradients computed for each y in <code>ys</code>.</li>
<li><b><code>name</code></b>: Optional name to use for grouping all the gradient ops together.
defaults to &lsquo;gradients&rsquo;.</li>
<li><b><code>colocate_gradients_with_ops</code></b>: If True, try colocating gradients with
the corresponding op.</li>
<li><b><code>gate_gradients</code></b>: If True, add a tuple around the gradients returned
for an operations.  This avoids some race conditions.</li>
<li><b><code>aggregation_method</code></b>: Specifies the method used to combine gradient terms.
Accepted values are constants defined in the class <code>AggregationMethod</code>.</li>
<li><b><code>stop_gradients</code></b>: Optional. A <code>Tensor</code> or list of tensors not to differentiate
through.</li>
<li><b><code>unconnected_gradients</code></b>: Optional. Specifies the gradient value returned when
the given input tensors are unconnected. Accepted values are constants
defined in the class <a href="../../../tf/UnconnectedGradients.html"><code>tf.UnconnectedGradients</code></a> and the default value is
<code>none</code>.</li>
</ul>


<h4>Returns:</h4>

<p>A list of <code>sum(dy/dx)</code> for each x in <code>xs</code>.</p>

<h4>Raises:</h4>

<ul>
<li><b><code>LookupError</code></b>: if one of the operations between <code>x</code> and <code>y</code> does not
have a registered gradient function.</li>
<li><b><code>ValueError</code></b>: if the arguments are invalid.</li>
<li><b><code>RuntimeError</code></b>: if called in Eager mode.</li>
</ul>

