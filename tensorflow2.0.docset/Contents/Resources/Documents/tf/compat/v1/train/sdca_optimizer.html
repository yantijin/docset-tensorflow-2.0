<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.compat.v1.train.sdca_optimizer" />
<meta itemprop="path" content="Stable" />
</div>


<h1>tf.compat.v1.train.sdca_optimizer</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p>Defined in generated file: <code>python/ops/gen_sdca_ops.py</code></p>

<!-- Start diff -->


<p>Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for</p>

<p><code>python
tf.compat.v1.train.sdca_optimizer(
    sparse_example_indices,
    sparse_feature_indices,
    sparse_feature_values,
    dense_features,
    example_weights,
    example_labels,
    sparse_indices,
    sparse_weights,
    dense_weights,
    example_state_data,
    loss_type,
    l1,
    l2,
    num_loss_partitions,
    num_inner_iterations,
    adaptative=True,
    name=None
)
</code></p>

<!-- Placeholder for "Used in" -->


<p>linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate.</p>

<p><a href="http://arxiv.org/pdf/1211.2717v1.pdf">Proximal Stochastic Dual Coordinate Ascent</a>.<br>
Shai Shalev-Shwartz, Tong Zhang. 2012</p>

<p>$$Loss Objective = \sum f<em>{i} (wx</em>{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$</p>

<p><a href="http://arxiv.org/abs/1502.03508">Adding vs. Averaging in Distributed Primal-Dual Optimization</a>.<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015</p>

<p><a href="https://arxiv.org/abs/1502.08053">Stochastic Dual Coordinate Ascent with Adaptive Probabilities</a>.<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015</p>

<h4>Args:</h4>

<ul>
<li><b><code>sparse_example_indices</code></b>: A list of <code>Tensor</code> objects with type <code>int64</code>.
a list of vectors which contain example indices.</li>
<li><b><code>sparse_feature_indices</code></b>: A list with the same length as <code>sparse_example_indices</code> of <code>Tensor</code> objects with type <code>int64</code>.
a list of vectors which contain feature indices.</li>
<li><b><code>sparse_feature_values</code></b>: A list of <code>Tensor</code> objects with type <code>float32</code>.
a list of vectors which contains feature value
associated with each feature group.</li>
<li><b><code>dense_features</code></b>: A list of <code>Tensor</code> objects with type <code>float32</code>.
a list of matrices which contains the dense feature values.</li>
<li><b><code>example_weights</code></b>: A <code>Tensor</code> of type <code>float32</code>.
a vector which contains the weight associated with each
example.</li>
<li><b><code>example_labels</code></b>: A <code>Tensor</code> of type <code>float32</code>.
a vector which contains the label/target associated with each
example.</li>
<li><b><code>sparse_indices</code></b>: A list with the same length as <code>sparse_example_indices</code> of <code>Tensor</code> objects with type <code>int64</code>.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach.</li>
<li><b><code>sparse_weights</code></b>: A list with the same length as <code>sparse_example_indices</code> of <code>Tensor</code> objects with type <code>float32</code>.
a list of vectors where each value is the weight associated with
a sparse feature group.</li>
<li><b><code>dense_weights</code></b>: A list with the same length as <code>dense_features</code> of <code>Tensor</code> objects with type <code>float32</code>.
a list of vectors where the values are the weights associated
with a dense feature group.</li>
<li><b><code>example_state_data</code></b>: A <code>Tensor</code> of type <code>float32</code>.
a list of vectors containing the example state data.</li>
<li><b><code>loss_type</code></b>: A <code>string</code> from: <code>"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"</code>.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses.</li>
<li><b><code>l1</code></b>: A <code>float</code>. Symmetric l1 regularization strength.</li>
<li><b><code>l2</code></b>: A <code>float</code>. Symmetric l2 regularization strength.</li>
<li><b><code>num_loss_partitions</code></b>: An <code>int</code> that is <code>&gt;= 1</code>.
Number of partitions of the global loss function.</li>
<li><b><code>num_inner_iterations</code></b>: An <code>int</code> that is <code>&gt;= 1</code>.
Number of iterations per mini-batch.</li>
<li><b><code>adaptative</code></b>: An optional <code>bool</code>. Defaults to <code>True</code>.
Whether to use Adaptive SDCA for the inner loop.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>


<h4>Returns:</h4>

<p>A tuple of <code>Tensor</code> objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights).</p>

<ul>
<li><b><code>out_example_state_data</code></b>: A <code>Tensor</code> of type <code>float32</code>.</li>
<li><b><code>out_delta_sparse_weights</code></b>: A list with the same length as <code>sparse_example_indices</code> of <code>Tensor</code> objects with type <code>float32</code>.</li>
<li><b><code>out_delta_dense_weights</code></b>: A list with the same length as <code>dense_features</code> of <code>Tensor</code> objects with type <code>float32</code>.</li>
</ul>

