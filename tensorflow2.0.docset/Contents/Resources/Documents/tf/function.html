
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href="../../default.css" rel="stylesheet">
    <link href="
   ../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.function" />
<meta itemprop="path" content="Stable" />
</div>

<h1 id="tffunction">tf.function</h1>
<!-- Insert buttons -->

<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/eager/def_function.py">View source</a></p>
<!-- Start diff -->

<p>Creates a callable TensorFlow graph from a Python function.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v1.function</code></li>
<li><code>tf.compat.v2.function</code></li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
    <span class="n">func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">input_signature</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">autograph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">experimental_autograph_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">experimental_relax_shapes</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>


<!-- Placeholder for "Used in" -->

<p><code>function</code> constructs a callable that executes a TensorFlow graph
(<a href="../tf/Graph.html"><code>tf.Graph</code></a>) created by tracing the TensorFlow operations in <code>func</code>.
This allows the TensorFlow runtime to apply optimizations and exploit
parallelism in the computation defined by <code>func</code>.</p>
<p><em>Example Usage</em></p>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">]])</span>

<span class="c1"># `f` and `g` will return the same value, but `g` will be executed as a</span>
<span class="c1"># TensorFlow graph.</span>
<span class="k">assert</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">==</span> <span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># Tensors and tf.Variables used by the Python function are captured in the</span>
<span class="c1"># graph.</span>
<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">h</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">assert</span> <span class="p">(</span><span class="n">h</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">==</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>

<span class="c1"># Data-dependent control flow is also captured in the graph. Supported</span>
<span class="c1"># control flow statements include `if`, `for`, `while`, `break`, `continue`,</span>
<span class="c1"># `return`.</span>
<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">x</span> <span class="o">//</span> <span class="mi">2</span>

<span class="c1"># print and TensorFlow side effects are supported, but exercise caution when</span>
<span class="c1"># using Python side effects like mutating objects, saving to files, etc.</span>
<span class="n">l</span> <span class="o">=</span> <span class="p">[]</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>                              <span class="c1"># Works</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>                       <span class="c1"># Works</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">py_func</span><span class="p">(</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">l</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">))(</span><span class="n">i</span><span class="p">)</span>  <span class="c1"># Works</span>
    <span class="n">l</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>                           <span class="c1"># Caution! Doesn&#39;t work.</span>
</pre></div>


<p>Note that unlike other TensorFlow operations, we don't convert python
numerical inputs to tensors. Moreover, a new graph is generated for each
distinct python numerical value, for example calling <code>g(2)</code> and <code>g(3)</code> will
generate two new graphs (while only one is generated if you call
<code>g(tf.constant(2))</code> and <code>g(tf.constant(3))</code>). Therefore, python numerical
inputs should be restricted to arguments that will have few distinct values,
such as hyperparameters like the number of layers in a neural network. This
allows TensorFlow to optimize each variant of the neural network.</p>
<p><em>Referencing <a href="../tf/Variable.html"><code>tf.Variable</code></a>s</em></p>
<p>The Python function <code>func</code> may reference stateful objects (such as
<a href="../tf/Variable.html"><code>tf.Variable</code></a>).
These are captured as implicit inputs to the callable returned by <code>function</code>.
For example:</p>
<div class="codehilite"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="n">c</span><span class="o">.</span><span class="n">assign_add</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

<span class="k">assert</span> <span class="nb">int</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
<span class="k">assert</span> <span class="n">f</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">==</span> <span class="mf">2.0</span>
<span class="k">assert</span> <span class="nb">int</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
<span class="k">assert</span> <span class="n">f</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">==</span> <span class="mf">3.0</span>
<span class="k">assert</span> <span class="nb">int</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
</pre></div>


<p><code>function</code> can be applied to methods of an object. For example:</p>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">Dense</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">glorot_uniform_initializer</span><span class="p">()((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

  <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
  <span class="k">def</span> <span class="nf">compute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>

<span class="n">d1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">()</span>
<span class="n">d2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="c1"># d1 and d2 are using distinct variables</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">d1</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">==</span> <span class="n">d2</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>


<p><em>Usage with <a href="../tf/keras.html"><code>tf.keras</code></a></em></p>
<p>The <code>call</code> methods of a <a href="../tf/keras/Model.html"><code>tf.keras.Model</code></a> subclass can be decorated with
<code>function</code> in order to apply graph execution optimizations on it.
For example:</p>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_probability</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MyModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">keep_probability</span> <span class="o">=</span> <span class="n">keep_probability</span>

  <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">training</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_probability</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">y</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># executes a graph, with dropout</span>
<span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># executes a graph, without dropout</span>
</pre></div>


<p><em>Input Signatures</em></p>
<p><code>function</code> instantiates a separate graph for every unique set of input
shapes and datatypes. For example, the following code snippet will result
in three distinct graphs being traced, as each input has a different
shape.</p>
<div class="codehilite"><pre><span></span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>

<span class="n">scalar</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">matrix</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">3.0</span><span class="p">]])</span>

<span class="n">f</span><span class="p">(</span><span class="n">scalar</span><span class="p">)</span>
<span class="n">f</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
<span class="n">f</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
</pre></div>


<p>An "input signature" can be optionally provided to <code>function</code> to control
the graphs traced. The input signature specifies the shape and type of each
<code>Tensor</code> argument to the function using a <a href="../tf/TensorSpec.html"><code>tf.TensorSpec</code></a> object. For example,
the following code snippet ensures that a single graph is created where the
input <code>Tensor</code> is required to be a floating point tensor with no restrictions
on shape.</p>
<div class="codehilite"><pre><span></span><span class="nd">@tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">input_signature</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)])</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
</pre></div>


<p>When an <code>input_signature</code> is specified, the callable will convert the inputs
to the specified TensorSpecs.</p>
<p><em>Tracing and staging</em></p>
<p>When <code>autograph</code> is <code>True</code>, all Python control flow that depends on <code>Tensor</code>
values is staged into a TensorFlow graph. When <code>autograph</code> is <code>False</code>, the
function is traced and control flow is not allowed to depend on data.</p>
<p>Note that <code>function</code> only stages TensorFlow operations, all Python code that
<code>func</code> executes and does not depend on data will shape the <em>construction</em> of
the graph.
For example, consider the following:</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">add_noise</span><span class="p">():</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">traced</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">add_noise</span><span class="p">)</span>
</pre></div>


<p><code>add_noise()</code> will return a different output every time it is invoked.
However, <code>traced()</code> will return the same value every time it is called,
since a particular random value generated by the <code>np.random.randn</code> call will
be inserted in the traced/staged TensorFlow graph as a constant. In this
particular example, replacing <code>np.random.randn(5, 5)</code> with
<code>tf.random.normal((5, 5))</code> will result in the same behavior for <code>add_noise()</code>
and <code>traced()</code>.</p>
<p><em>Python Side-Effects</em></p>
<p>A corollary of the previous discussion on tracing is the following: If a
Python function <code>func</code> has Python side-effects, then executing <code>func</code> multiple
times may not be semantically equivalent to executing <code>F = tf.function(func)</code>
multiple times; this difference is due to the fact that <code>function</code> only
captures the subgraph of TensorFlow operations that is constructed when <code>func</code>
is invoked to trace a graph.</p>
<p>The same is true if code with Python side effects is used inside control flow,
such as a loop. If your code uses side effects that are not intended to
control graph construction, wrap them inside <a href="../tf/compat/v1/py_func.html"><code>tf.compat.v1.py_func</code></a>.</p>
<p><em>Retracing</em></p>
<p>A single tf.function object might need to map to multiple computation graphs
under the hood. This should be visible only as performance (tracing graphs has
a nonzero computational and memory cost) but should not affect the correctness
of the program. A traced function should return the same result as it would
when run eagerly, assuming no unintended Python side-effects.</p>
<p>Calling a <a href="../tf/function.html"><code>tf.function</code></a> with tensor arguments of different dtypes should lead
to at least one computational graph per distinct set of dtypes. Alternatively,
always calling a <a href="../tf/function.html"><code>tf.function</code></a> with tensor arguments of the same shapes and
dtypes and the same non-tensor arguments should not lead to additional
retracings of your function.</p>
<p>Other than that, TensorFlow reserves the right to retrace functions as many
times as needed, to ensure that traced functions behave as they would when run
eagerly and to provide the best end-to-end performance. For example, the
behavior of how many traces TensorFlow will do when the function is repeatedly
called with different python scalars as arguments is left undefined to allow
for future optimizations.</p>
<p>To control the tracing behavior, use the following tools:
 - different <a href="../tf/function.html"><code>tf.function</code></a> objects are guaranteed to not share traces; and
 - specifying a signature or using concrete function objects returned from
   get_concrete_function() guarantees that only one function graph will be
   built.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>func</code></b>: function to be compiled. If <code>func</code> is None, returns a decorator that
  can be invoked with a single argument - <code>func</code>. The end result is
  equivalent to providing all the arguments up front. In other words,
  <code>tf.function(input_signature=...)(func)</code> is equivalent to
  <a href="../tf/function.html"><code>tf.function(func, input_signature=...)</code></a>. The former can be used to
  decorate Python functions, for example:
    @tf.function(input_signature=...)
    def foo(...): ...</li>
<li><b><code>input_signature</code></b>: A possibly nested sequence of <a href="../tf/TensorSpec.html"><code>tf.TensorSpec</code></a> objects
  specifying the shapes and dtypes of the Tensors that will be supplied to
  this function. If <code>None</code>, a separate function is instantiated for each
  inferred input signature.  If input_signature is specified, every input to
  <code>func</code> must be a <code>Tensor</code>, and <code>func</code> cannot accept <code>**kwargs</code>.</li>
<li><b><code>autograph</code></b>: Whether autograph should be applied on <code>func</code> before tracing a
  graph. This allows for dynamic control flow (Python if's, loops etc.)
  in the traced graph. See https://www.tensorflow.org/guide/autograph for
    more information.</li>
<li><b><code>experimental_autograph_options</code></b>: Experimental knobs (in the form of a tuple
  of tensorflow.autograph.Feature values) to control behavior when
  autograph=True.</li>
<li><b><code>experimental_relax_shapes</code></b>: When true, argument shapes may be relaxed to
  avoid unecessary retracing.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>If <code>func</code> is not None, returns a callable that will execute the compiled
function (and return zero or more <a href="../tf/Tensor.html"><code>tf.Tensor</code></a> objects).
If <code>func</code> is None, returns a decorator that, when invoked with a single
<code>func</code> argument, returns a callable equivalent to the case above.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If <code>input_signature</code> is neither <code>None</code> nor a sequence of
  <code>TensorSpec</code> objects.</li>
</ul>
    </body>
    </html>
   