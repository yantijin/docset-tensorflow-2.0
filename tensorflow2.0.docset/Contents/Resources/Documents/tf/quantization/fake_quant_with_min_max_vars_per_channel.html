
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href=../../../default.css" rel="stylesheet">
    <link href="
   ../../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.quantization.fake_quant_with_min_max_vars_per_channel" />
<meta itemprop="path" content="Stable" />
</div>

<h1 id="tfquantizationfake_quant_with_min_max_vars_per_channel">tf.quantization.fake_quant_with_min_max_vars_per_channel</h1>
<!-- Insert buttons -->

<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p>Defined in generated file: <code>python/ops/gen_array_ops.py</code></p>
<!-- Start diff -->

<p>Fake-quantize the 'inputs' tensor of type float and one of the shapes: <code>[d]</code>,</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v1.fake_quant_with_min_max_vars_per_channel</code></li>
<li><code>tf.compat.v1.quantization.fake_quant_with_min_max_vars_per_channel</code></li>
<li><code>tf.compat.v2.quantization.fake_quant_with_min_max_vars_per_channel</code></li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">fake_quant_with_min_max_vars_per_channel</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="nb">min</span><span class="p">,</span>
    <span class="nb">max</span><span class="p">,</span>
    <span class="n">num_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">narrow_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<!-- Placeholder for "Used in" -->

<p><code>[b, d]</code> <code>[b, h, w, d]</code> via per-channel floats <code>min</code> and <code>max</code> of shape <code>[d]</code>
to 'outputs' tensor of same shape as <code>inputs</code>.</p>
<p><code>[min; max]</code> define the clamping range for the <code>inputs</code> data.
<code>inputs</code> values are quantized into the quantization range (<code>[0; 2^num_bits - 1]</code>
when <code>narrow_range</code> is false and <code>[1; 2^num_bits - 1]</code> when it is true) and
then de-quantized and output as floats in <code>[min; max]</code> interval.
<code>num_bits</code> is the bitwidth of the quantization; between 2 and 16, inclusive.</p>
<p>Before quantization, <code>min</code> and <code>max</code> values are adjusted with the following
logic.
It is suggested to have <code>min &lt;= 0 &lt;= max</code>. If <code>0</code> is not in the range of values,
the behavior can be unexpected:
If <code>0 &lt; min &lt; max</code>: <code>min_adj = 0</code> and <code>max_adj = max - min</code>.
If <code>min &lt; max &lt; 0</code>: <code>min_adj = min - max</code> and <code>max_adj = 0</code>.
If <code>min &lt;= 0 &lt;= max</code>: <code>scale = (max - min) / (2^num_bits - 1)</code>,
<code>min_adj = scale * round(min / scale)</code> and <code>max_adj = max + min_adj - min</code>.</p>
<p>This operation has a gradient and thus allows for training <code>min</code> and <code>max</code>
values.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>inputs</code></b>: A <code>Tensor</code> of type <code>float32</code>.</li>
<li><b><code>min</code></b>: A <code>Tensor</code> of type <code>float32</code>.</li>
<li><b><code>max</code></b>: A <code>Tensor</code> of type <code>float32</code>.</li>
<li><b><code>num_bits</code></b>: An optional <code>int</code>. Defaults to <code>8</code>.</li>
<li><b><code>narrow_range</code></b>: An optional <code>bool</code>. Defaults to <code>False</code>.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A <code>Tensor</code> of type <code>float32</code>.</p>
    </body>
    </html>
   