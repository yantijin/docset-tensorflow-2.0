<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.nn.embedding_lookup_sparse" />
<meta itemprop="path" content="Stable" />
</div>


<h1>tf.nn.embedding_lookup_sparse</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/ops/embedding_ops.py">View source</a></p>

<!-- Start diff -->


<p>Computes embeddings for the given ids and weights.</p>

<h3>Aliases:</h3>

<ul>
<li><code>tf.compat.v2.nn.embedding_lookup_sparse</code></li>
</ul>


<p><code>python
tf.nn.embedding_lookup_sparse(
    params,
    sp_ids,
    sp_weights,
    combiner=None,
    max_norm=None,
    name=None
)
</code></p>

<!-- Placeholder for "Used in" -->


<p>This op assumes that there is at least one id for each row in the dense tensor
represented by sp_ids (i.e. there are no rows with empty features), and that
all the indices of sp_ids are in canonical row-major order.</p>

<p>It also assumes that all id values lie in the range [0, p0), where p0
is the sum of the size of params along dimension 0.</p>

<h4>Args:</h4>

<ul>
<li><b><code>params</code></b>: A single tensor representing the complete embedding tensor, or a
list of P tensors all of same shape except for the first dimension,
representing sharded embedding tensors.  Alternatively, a
<code>PartitionedVariable</code>, created by partitioning along dimension 0. Each
element must be appropriately sized for <code>"div"</code> <code>partition_strategy</code>.</li>
<li><b><code>sp_ids</code></b>: N x M <code>SparseTensor</code> of int64 ids where N is typically batch size
and M is arbitrary.</li>
<li><b><code>sp_weights</code></b>: either a <code>SparseTensor</code> of float / double weights, or <code>None</code> to
indicate all weights should be taken to be 1. If specified, <code>sp_weights</code>
must have exactly the same shape and indices as <code>sp_ids</code>.</li>
<li><b><code>combiner</code></b>: A string specifying the reduction op. Currently &ldquo;mean&rdquo;, &ldquo;sqrtn&rdquo;
and &ldquo;sum&rdquo; are supported. &ldquo;sum&rdquo; computes the weighted sum of the embedding
results for each row. &ldquo;mean&rdquo; is the weighted sum divided by the total
weight. &ldquo;sqrtn&rdquo; is the weighted sum divided by the square root of the sum
of the squares of the weights.</li>
<li><b><code>max_norm</code></b>: If not <code>None</code>, each embedding is clipped if its l2-norm is larger
than this value, before combining.</li>
<li><b><code>name</code></b>: Optional name for the op.</li>
</ul>


<h4>Returns:</h4>

<p>A dense tensor representing the combined embeddings for the
sparse ids. For each row in the dense tensor represented by <code>sp_ids</code>, the op
looks up the embeddings for all ids in that row, multiplies them by the
corresponding weight, and combines these embeddings as specified.</p>

<p>In other words, if</p>

<p>  <code>shape(combined params) = [p0, p1, ..., pm]</code></p>

<p>and</p>

<p>  <code>shape(sp_ids) = shape(sp_weights) = [d0, d1, ..., dn]</code></p>

<p>then</p>

<p>  <code>shape(output) = [d0, d1, ..., dn-1, p1, ..., pm]</code>.</p>

<p>For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are</p>

<p>  <code>python
 </code></p>

<p>with <code>combiner</code>=&ldquo;mean&rdquo;, then the output will be a 3x20 matrix where</p>

<p>  <code>python
  output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)
  output[1, :] = (params[0, :] * 1.0) / 1.0
  output[2, :] = (params[1, :] * 3.0) / 3.0
 </code></p>

<h4>Raises:</h4>

<ul>
<li><b><code>TypeError</code></b>: If <code>sp_ids</code> is not a <code>SparseTensor</code>, or if <code>sp_weights</code> is
neither <code>None</code> nor <code>SparseTensor</code>.</li>
<li><b><code>ValueError</code></b>: If <code>combiner</code> is not one of {&ldquo;mean&rdquo;, &ldquo;sqrtn&rdquo;, &ldquo;sum&rdquo;}.</li>
</ul>

