<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.keras.backend.batch_dot" />
<meta itemprop="path" content="Stable" />
</div>


<h1>tf.keras.backend.batch_dot</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/backend.py">View source</a></p>

<!-- Start diff -->


<p>Batchwise dot product.</p>

<h3>Aliases:</h3>

<ul>
<li><code>tf.compat.v1.keras.backend.batch_dot</code></li>
<li><code>tf.compat.v2.keras.backend.batch_dot</code></li>
</ul>


<p><code>python
tf.keras.backend.batch_dot(
    x,
    y,
    axes=None
)
</code></p>

<!-- Placeholder for "Used in" -->


<p><code>batch_dot</code> is used to compute dot product of <code>x</code> and <code>y</code> when
<code>x</code> and <code>y</code> are data in batch, i.e. in a shape of
<code>(batch_size, :)</code>.
<code>batch_dot</code> results in a tensor or variable with less dimensions
than the input. If the number of dimensions is reduced to 1,
we use <code>expand_dims</code> to make sure that ndim is at least 2.</p>

<h4>Arguments:</h4>

<ul>
<li><b><code>x</code></b>: Keras tensor or variable with <code>ndim &gt;= 2</code>.</li>
<li><b><code>y</code></b>: Keras tensor or variable with <code>ndim &gt;= 2</code>.</li>
<li><b><code>axes</code></b>: Tuple or list of integers with target dimensions, or single integer.
The sizes of <code>x.shape[axes[0]]</code> and <code>y.shape[axes[1]]</code> should be equal.</li>
</ul>


<h4>Returns:</h4>

<p>A tensor with shape equal to the concatenation of <code>x</code>&rsquo;s shape
(less the dimension that was summed over) and <code>y</code>&rsquo;s shape
(less the batch dimension and the dimension that was summed over).
If the final rank is 1, we reshape it to <code>(batch_size, 1)</code>.</p>

<h4>Examples:</h4>

<p>Assume <code>x = [[1, 2], [3, 4]]</code> and <code>y = [[5, 6], [7, 8]]</code>
<code>batch_dot(x, y, axes=1) = [[17], [53]]</code> which is the main diagonal
of <code>x.dot(y.T)</code>, although we never have to calculate the off-diagonal
elements.</p>

<ul>
<li><b><code>Pseudocode</code></b>: <code>
inner_products = []
for xi, yi in zip(x, y):
  inner_products.append(xi.dot(yi))
result = stack(inner_products)
</code></li>
</ul>


<p>Shape inference:
Let <code>x</code>&rsquo;s shape be <code>(100, 20)</code> and <code>y</code>&rsquo;s shape be <code>(100, 30, 20)</code>.
If <code>axes</code> is (1, 2), to find the output shape of resultant tensor,
    loop through each dimension in <code>x</code>&rsquo;s shape and <code>y</code>&rsquo;s shape:
* <code>x.shape[0]</code> : 100 : append to output shape
* <code>x.shape[1]</code> : 20 : do not append to output shape,
    dimension 1 of <code>x</code> has been summed over. (<code>dot_axes[0]</code> = 1)
* <code>y.shape[0]</code> : 100 : do not append to output shape,
    always ignore first dimension of <code>y</code>
* <code>y.shape[1]</code> : 30 : append to output shape
* <code>y.shape[2]</code> : 20 : do not append to output shape,
    dimension 2 of <code>y</code> has been summed over. (<code>dot_axes[1]</code> = 2)
<code>output_shape</code> = <code>(100, 30)</code></p>

<blockquote><blockquote><blockquote><p>x_batch = tf.keras.backend.ones(shape=(32, 20, 1))
y_batch = tf.keras.backend.ones(shape=(32, 30, 20))
xy_batch_dot = tf.keras.backend.batch_dot(x_batch, y_batch, axes=(1, 2))
tf.keras.backend.int_shape(xy_batch_dot)
(32, 1, 30)</p></blockquote></blockquote></blockquote>
