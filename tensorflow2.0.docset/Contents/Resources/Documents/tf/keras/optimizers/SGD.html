
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href="../../../../default.css" rel="stylesheet">
    <link href="
   ../../../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.keras.optimizers.SGD" />
<meta itemprop="path" content="Stable" />
<meta itemprop="property" content="iterations"/>
<meta itemprop="property" content="weights"/>
<meta itemprop="property" content="__init__"/>
<meta itemprop="property" content="add_slot"/>
<meta itemprop="property" content="add_weight"/>
<meta itemprop="property" content="apply_gradients"/>
<meta itemprop="property" content="from_config"/>
<meta itemprop="property" content="get_config"/>
<meta itemprop="property" content="get_gradients"/>
<meta itemprop="property" content="get_slot"/>
<meta itemprop="property" content="get_slot_names"/>
<meta itemprop="property" content="get_updates"/>
<meta itemprop="property" content="get_weights"/>
<meta itemprop="property" content="minimize"/>
<meta itemprop="property" content="set_weights"/>
<meta itemprop="property" content="variables"/>
</div>

<h1 id="tfkerasoptimizerssgd">tf.keras.optimizers.SGD</h1>
<!-- Insert buttons -->

<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/gradient_descent.py">View source</a></p>
<h2 id="class-sgd">Class <code>SGD</code></h2>
<!-- Start diff -->

<p>Stochastic gradient descent and momentum optimizer.</p>
<p>Inherits From: <a href="../../../tf/keras/optimizers/Optimizer.html"><code>Optimizer</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.keras.optimizers.SGD</code></li>
<li>Class <code>tf.compat.v2.keras.optimizers.SGD</code></li>
<li>Class <code>tf.compat.v2.optimizers.SGD</code></li>
<li>Class <code>tf.optimizers.SGD</code></li>
</ul>
<!-- Placeholder for "Used in" -->

<h4 id="computes">Computes:</h4>
<div class="codehilite"><pre><span></span><span class="err">theta(t+1) = theta(t) - learning_rate * gradient</span>
<span class="err">gradient is evaluated at theta(t).</span>
</pre></div>


<p>or Computes (if <code>nesterov = False</code>):</p>
<div class="codehilite"><pre><span></span><span class="nf">v</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="n">momentum</span> <span class="o">*</span> <span class="nf">v</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span>
<span class="nf">theta</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="nf">theta</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">+</span> <span class="nf">v</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="k">if</span> <span class="ss">`nesterov`</span> <span class="k">is</span> <span class="no">False</span><span class="p">,</span> <span class="n">gradient</span> <span class="k">is</span> <span class="n">evaluated</span> <span class="n">at</span> <span class="nf">theta</span><span class="p">(</span><span class="n">t</span><span class="p">).</span>
<span class="k">if</span> <span class="ss">`nesterov`</span> <span class="k">is</span> <span class="no">True</span><span class="p">,</span> <span class="n">gradient</span> <span class="k">is</span> <span class="n">evaluated</span> <span class="n">at</span> <span class="nf">theta</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">+</span> <span class="n">momentum</span> <span class="o">*</span> <span class="nf">v</span><span class="p">(</span><span class="n">t</span><span class="p">),</span>
  <span class="k">and</span> <span class="n">the</span> <span class="n">variables</span> <span class="n">always</span> <span class="n">store</span> <span class="n">theta</span> <span class="o">+</span> <span class="n">m</span> <span class="n">v</span> <span class="n">instead</span> <span class="n">of</span> <span class="n">theta</span>
</pre></div>


<p>Some of the args below are hyperparameters, where a hyperparameter is
defined as a scalar Tensor, a regular Python value, or a callable (which
will be evaluated when <code>apply_gradients</code> is called) returning a scalar
Tensor or a Python value.</p>
<h1 id="references">References</h1>
<div class="codehilite"><pre><span></span><span class="err">nesterov = True, See [Sutskever et al., 2013](</span>
<span class="err">  http://jmlr.org/proceedings/papers/v28/sutskever13.pdf).</span>
</pre></div>


<h4 id="eager-compatibility">Eager Compatibility</h4>
<p>When eager execution is enabled, learning_rate can be a callable that takes
no arguments and returns the actual value to use. This can be useful for
changing these values across different invocations of optimizer functions.</p>
<h2 id="__init__"><code>__init__</code></h2>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/gradient_descent.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="fm">__init__</span><span class="p">(</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;SGD&#39;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>


<p>Construct a new Stochastic Gradient Descent or Momentum optimizer.</p>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>learning_rate</code></b>: float hyperparameter &gt;= 0. Learning rate.</li>
<li><b><code>momentum</code></b>: float hyperparameter &gt;= 0 that accelerates SGD in the relevant
  direction and dampens oscillations.</li>
<li><b><code>nesterov</code></b>: boolean. Whether to apply Nesterov momentum.</li>
<li><b><code>name</code></b>: Optional name prefix for the operations created when applying
  gradients.  Defaults to 'SGD'.</li>
<li><b><code>**kwargs</code></b>: keyword arguments. Allowed to be {<code>clipnorm</code>, <code>clipvalue</code>, <code>lr</code>,
  <code>decay</code>}. <code>clipnorm</code> is clip gradients by norm; <code>clipvalue</code> is clip
  gradients by value, <code>decay</code> is included for backward compatibility to
  allow time inverse decay of learning rate. <code>lr</code> is included for backward
  compatibility, recommended to use <code>learning_rate</code> instead.</li>
</ul>
<h2 id="properties">Properties</h2>
<h3 id="iterations"><code>iterations</code></h3>

<p>Variable. The number of training steps this Optimizer has run.</p>
<h3 id="weights"><code>weights</code></h3>

<p>Returns variables of this Optimizer based on the order created.</p>
<h2 id="methods">Methods</h2>
<h3 id="add_slot"><code>add_slot</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">add_slot</span><span class="p">(</span>
    <span class="n">var</span><span class="p">,</span>
    <span class="n">slot_name</span><span class="p">,</span>
    <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span>
<span class="p">)</span>
</pre></div>


<p>Add a new slot variable for <code>var</code>.</p>
<h3 id="add_weight"><code>add_weight</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">add_weight</span><span class="p">(</span>
    <span class="n">name</span><span class="p">,</span>
    <span class="n">shape</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">initializer</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
    <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">synchronization</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
    <span class="n">aggregation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span>
<span class="p">)</span>
</pre></div>


<h3 id="apply_gradients"><code>apply_gradients</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">apply_gradients</span><span class="p">(</span>
    <span class="n">grads_and_vars</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Apply gradients to variables.</p>
<p>This is the second part of <code>minimize()</code>. It returns an <code>Operation</code> that
applies gradients.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>grads_and_vars</code></b>: List of (gradient, variable) pairs.</li>
<li><b><code>name</code></b>: Optional name for the returned operation.  Default to the name
  passed to the <code>Optimizer</code> constructor.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>An <code>Operation</code> that applies the specified gradients. If <code>global_step</code>
was not None, that operation also increments <code>global_step</code>.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If <code>grads_and_vars</code> is malformed.</li>
<li><b><code>ValueError</code></b>: If none of the variables have gradients.</li>
</ul>
<h3 id="from_config"><code>from_config</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">from_config</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">config</span><span class="p">,</span>
    <span class="n">custom_objects</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Creates an optimizer from its config.</p>
<p>This method is the reverse of <code>get_config</code>,
capable of instantiating the same optimizer from the config
dictionary.</p>
<h4 id="arguments_1">Arguments:</h4>
<ul>
<li><b><code>config</code></b>: A Python dictionary, typically the output of get_config.</li>
<li><b><code>custom_objects</code></b>: A Python dictionary mapping names to additional Python
  objects used to create this optimizer, such as a function used for a
  hyperparameter.</li>
</ul>
<h4 id="returns_1">Returns:</h4>
<p>An optimizer instance.</p>
<h3 id="get_config"><code>get_config</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/gradient_descent.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">get_config</span><span class="p">()</span>
</pre></div>


<p>Returns the config of the optimimizer.</p>
<p>An optimizer config is a Python dictionary (serializable)
containing the configuration of an optimizer.
The same optimizer can be reinstantiated later
(without any saved state) from this configuration.</p>
<h4 id="returns_2">Returns:</h4>
<p>Python dictionary.</p>
<h3 id="get_gradients"><code>get_gradients</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">get_gradients</span><span class="p">(</span>
    <span class="n">loss</span><span class="p">,</span>
    <span class="n">params</span>
<span class="p">)</span>
</pre></div>


<p>Returns gradients of <code>loss</code> with respect to <code>params</code>.</p>
<h4 id="arguments_2">Arguments:</h4>
<ul>
<li><b><code>loss</code></b>: Loss tensor.</li>
<li><b><code>params</code></b>: List of variables.</li>
</ul>
<h4 id="returns_3">Returns:</h4>
<p>List of gradient tensors.</p>
<h4 id="raises_1">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: In case any gradient cannot be computed (e.g. if gradient
  function not implemented).</li>
</ul>
<h3 id="get_slot"><code>get_slot</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">get_slot</span><span class="p">(</span>
    <span class="n">var</span><span class="p">,</span>
    <span class="n">slot_name</span>
<span class="p">)</span>
</pre></div>


<h3 id="get_slot_names"><code>get_slot_names</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">get_slot_names</span><span class="p">()</span>
</pre></div>


<p>A list of names for this optimizer's slots.</p>
<h3 id="get_updates"><code>get_updates</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">get_updates</span><span class="p">(</span>
    <span class="n">loss</span><span class="p">,</span>
    <span class="n">params</span>
<span class="p">)</span>
</pre></div>


<h3 id="get_weights"><code>get_weights</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">get_weights</span><span class="p">()</span>
</pre></div>


<h3 id="minimize"><code>minimize</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">minimize</span><span class="p">(</span>
    <span class="n">loss</span><span class="p">,</span>
    <span class="n">var_list</span><span class="p">,</span>
    <span class="n">grad_loss</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Minimize <code>loss</code> by updating <code>var_list</code>.</p>
<p>This method simply computes gradient using <a href="../../../tf/GradientTape.html"><code>tf.GradientTape</code></a> and calls
<code>apply_gradients()</code>. If you want to process the gradient before applying
then call <a href="../../../tf/GradientTape.html"><code>tf.GradientTape</code></a> and <code>apply_gradients()</code> explicitly instead
of using this function.</p>
<h4 id="args_1">Args:</h4>
<ul>
<li><b><code>loss</code></b>: A callable taking no arguments which returns the value to minimize.</li>
<li><b><code>var_list</code></b>: list or tuple of <code>Variable</code> objects to update to minimize
  <code>loss</code>, or a callable returning the list or tuple of <code>Variable</code> objects.
  Use callable when the variable list would otherwise be incomplete before
  <code>minimize</code> since the variables are created at the first time <code>loss</code> is
  called.</li>
<li><b><code>grad_loss</code></b>: Optional. A <code>Tensor</code> holding the gradient computed for <code>loss</code>.</li>
<li><b><code>name</code></b>: Optional name for the returned operation.</li>
</ul>
<h4 id="returns_4">Returns:</h4>
<p>An Operation that updates the variables in <code>var_list</code>.  If <code>global_step</code>
was not <code>None</code>, that operation also increments <code>global_step</code>.</p>
<h4 id="raises_2">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If some of the variables are not <code>Variable</code> objects.</li>
</ul>
<h3 id="set_weights"><code>set_weights</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">set_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
</pre></div>


<h3 id="variables"><code>variables</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">variables</span><span class="p">()</span>
</pre></div>


<p>Returns variables of this Optimizer based on the order created.</p>
    </body>
    </html>
   