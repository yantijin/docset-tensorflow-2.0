<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.keras.optimizers.SGD" />
<meta itemprop="path" content="Stable" />
<meta itemprop="property" content="iterations"/>
<meta itemprop="property" content="weights"/>
<meta itemprop="property" content="__init__"/>
<meta itemprop="property" content="add_slot"/>
<meta itemprop="property" content="add_weight"/>
<meta itemprop="property" content="apply_gradients"/>
<meta itemprop="property" content="from_config"/>
<meta itemprop="property" content="get_config"/>
<meta itemprop="property" content="get_gradients"/>
<meta itemprop="property" content="get_slot"/>
<meta itemprop="property" content="get_slot_names"/>
<meta itemprop="property" content="get_updates"/>
<meta itemprop="property" content="get_weights"/>
<meta itemprop="property" content="minimize"/>
<meta itemprop="property" content="set_weights"/>
<meta itemprop="property" content="variables"/>
</div>


<h1>tf.keras.optimizers.SGD</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/gradient_descent.py">View source</a></p>

<h2>Class <code>SGD</code></h2>

<!-- Start diff -->


<p>Stochastic gradient descent and momentum optimizer.</p>

<p>Inherits From: <a href="../../../tf/keras/optimizers/Optimizer.html"><code>Optimizer</code></a></p>

<h3>Aliases:</h3>

<ul>
<li>Class <code>tf.compat.v1.keras.optimizers.SGD</code></li>
<li>Class <code>tf.compat.v2.keras.optimizers.SGD</code></li>
<li>Class <code>tf.compat.v2.optimizers.SGD</code></li>
<li>Class <code>tf.optimizers.SGD</code></li>
</ul>


<!-- Placeholder for "Used in" -->


<h4>Computes:</h4>

<p><code>
theta(t+1) = theta(t) - learning_rate * gradient
gradient is evaluated at theta(t).
</code></p>

<p>or Computes (if <code>nesterov = False</code>):
<code>
v(t+1) = momentum * v(t) - learning_rate * gradient
theta(t+1) = theta(t) + v(t+1)
if `nesterov` is False, gradient is evaluated at theta(t).
if `nesterov` is True, gradient is evaluated at theta(t) + momentum * v(t),
  and the variables always store theta + m v instead of theta
</code></p>

<p>Some of the args below are hyperparameters, where a hyperparameter is
defined as a scalar Tensor, a regular Python value, or a callable (which
will be evaluated when <code>apply_gradients</code> is called) returning a scalar
Tensor or a Python value.</p>

<h1>References</h1>

<pre><code>nesterov = True, See [Sutskever et al., 2013](
  http://jmlr.org/proceedings/papers/v28/sutskever13.pdf).
</code></pre>

<h4>Eager Compatibility</h4>

<p>When eager execution is enabled, learning_rate can be a callable that takes
no arguments and returns the actual value to use. This can be useful for
changing these values across different invocations of optimizer functions.</p>

<h2 id="__init__"><code>__init__</code></h2>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/gradient_descent.py">View source</a></p>

<p><code>python
__init__(
    learning_rate=0.01,
    momentum=0.0,
    nesterov=False,
    name='SGD',
    **kwargs
)
</code></p>

<p>Construct a new Stochastic Gradient Descent or Momentum optimizer.</p>

<h4>Arguments:</h4>

<ul>
<li><b><code>learning_rate</code></b>: float hyperparameter >= 0. Learning rate.</li>
<li><b><code>momentum</code></b>: float hyperparameter >= 0 that accelerates SGD in the relevant
direction and dampens oscillations.</li>
<li><b><code>nesterov</code></b>: boolean. Whether to apply Nesterov momentum.</li>
<li><b><code>name</code></b>: Optional name prefix for the operations created when applying
gradients.  Defaults to &lsquo;SGD&rsquo;.</li>
<li><b><code>**kwargs</code></b>: keyword arguments. Allowed to be {<code>clipnorm</code>, <code>clipvalue</code>, <code>lr</code>,
<code>decay</code>}. <code>clipnorm</code> is clip gradients by norm; <code>clipvalue</code> is clip
gradients by value, <code>decay</code> is included for backward compatibility to
allow time inverse decay of learning rate. <code>lr</code> is included for backward
compatibility, recommended to use <code>learning_rate</code> instead.</li>
</ul>


<h2>Properties</h2>

<h3 id="iterations"><code>iterations</code></h3>


<p>Variable. The number of training steps this Optimizer has run.</p>

<h3 id="weights"><code>weights</code></h3>


<p>Returns variables of this Optimizer based on the order created.</p>

<h2>Methods</h2>

<h3 id="add_slot"><code>add_slot</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>

<p><code>python
add_slot(
    var,
    slot_name,
    initializer='zeros'
)
</code></p>

<p>Add a new slot variable for <code>var</code>.</p>

<h3 id="add_weight"><code>add_weight</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>

<p><code>python
add_weight(
    name,
    shape,
    dtype=None,
    initializer='zeros',
    trainable=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE
)
</code></p>

<h3 id="apply_gradients"><code>apply_gradients</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>

<p><code>python
apply_gradients(
    grads_and_vars,
    name=None
)
</code></p>

<p>Apply gradients to variables.</p>

<p>This is the second part of <code>minimize()</code>. It returns an <code>Operation</code> that
applies gradients.</p>

<h4>Args:</h4>

<ul>
<li><b><code>grads_and_vars</code></b>: List of (gradient, variable) pairs.</li>
<li><b><code>name</code></b>: Optional name for the returned operation.  Default to the name
passed to the <code>Optimizer</code> constructor.</li>
</ul>


<h4>Returns:</h4>

<p>An <code>Operation</code> that applies the specified gradients. If <code>global_step</code>
was not None, that operation also increments <code>global_step</code>.</p>

<h4>Raises:</h4>

<ul>
<li><b><code>TypeError</code></b>: If <code>grads_and_vars</code> is malformed.</li>
<li><b><code>ValueError</code></b>: If none of the variables have gradients.</li>
</ul>


<h3 id="from_config"><code>from_config</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>

<p><code>python
from_config(
    cls,
    config,
    custom_objects=None
)
</code></p>

<p>Creates an optimizer from its config.</p>

<p>This method is the reverse of <code>get_config</code>,
capable of instantiating the same optimizer from the config
dictionary.</p>

<h4>Arguments:</h4>

<ul>
<li><b><code>config</code></b>: A Python dictionary, typically the output of get_config.</li>
<li><b><code>custom_objects</code></b>: A Python dictionary mapping names to additional Python
objects used to create this optimizer, such as a function used for a
hyperparameter.</li>
</ul>


<h4>Returns:</h4>

<p>An optimizer instance.</p>

<h3 id="get_config"><code>get_config</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/gradient_descent.py">View source</a></p>

<p><code>python
get_config()
</code></p>

<p>Returns the config of the optimimizer.</p>

<p>An optimizer config is a Python dictionary (serializable)
containing the configuration of an optimizer.
The same optimizer can be reinstantiated later
(without any saved state) from this configuration.</p>

<h4>Returns:</h4>

<p>Python dictionary.</p>

<h3 id="get_gradients"><code>get_gradients</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>

<p><code>python
get_gradients(
    loss,
    params
)
</code></p>

<p>Returns gradients of <code>loss</code> with respect to <code>params</code>.</p>

<h4>Arguments:</h4>

<ul>
<li><b><code>loss</code></b>: Loss tensor.</li>
<li><b><code>params</code></b>: List of variables.</li>
</ul>


<h4>Returns:</h4>

<p>List of gradient tensors.</p>

<h4>Raises:</h4>

<ul>
<li><b><code>ValueError</code></b>: In case any gradient cannot be computed (e.g. if gradient
function not implemented).</li>
</ul>


<h3 id="get_slot"><code>get_slot</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>

<p><code>python
get_slot(
    var,
    slot_name
)
</code></p>

<h3 id="get_slot_names"><code>get_slot_names</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>

<p><code>python
get_slot_names()
</code></p>

<p>A list of names for this optimizer&rsquo;s slots.</p>

<h3 id="get_updates"><code>get_updates</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>

<p><code>python
get_updates(
    loss,
    params
)
</code></p>

<h3 id="get_weights"><code>get_weights</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>

<p><code>python
get_weights()
</code></p>

<h3 id="minimize"><code>minimize</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>

<p><code>python
minimize(
    loss,
    var_list,
    grad_loss=None,
    name=None
)
</code></p>

<p>Minimize <code>loss</code> by updating <code>var_list</code>.</p>

<p>This method simply computes gradient using <a href="../../../tf/GradientTape.html"><code>tf.GradientTape</code></a> and calls
<code>apply_gradients()</code>. If you want to process the gradient before applying
then call <a href="../../../tf/GradientTape.html"><code>tf.GradientTape</code></a> and <code>apply_gradients()</code> explicitly instead
of using this function.</p>

<h4>Args:</h4>

<ul>
<li><b><code>loss</code></b>: A callable taking no arguments which returns the value to minimize.</li>
<li><b><code>var_list</code></b>: list or tuple of <code>Variable</code> objects to update to minimize
<code>loss</code>, or a callable returning the list or tuple of <code>Variable</code> objects.
Use callable when the variable list would otherwise be incomplete before
<code>minimize</code> since the variables are created at the first time <code>loss</code> is
called.</li>
<li><b><code>grad_loss</code></b>: Optional. A <code>Tensor</code> holding the gradient computed for <code>loss</code>.</li>
<li><b><code>name</code></b>: Optional name for the returned operation.</li>
</ul>


<h4>Returns:</h4>

<p>An Operation that updates the variables in <code>var_list</code>.  If <code>global_step</code>
was not <code>None</code>, that operation also increments <code>global_step</code>.</p>

<h4>Raises:</h4>

<ul>
<li><b><code>ValueError</code></b>: If some of the variables are not <code>Variable</code> objects.</li>
</ul>


<h3 id="set_weights"><code>set_weights</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>

<p><code>python
set_weights(weights)
</code></p>

<h3 id="variables"><code>variables</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/optimizer_v2/optimizer_v2.py">View source</a></p>

<p><code>python
variables()
</code></p>

<p>Returns variables of this Optimizer based on the order created.</p>
