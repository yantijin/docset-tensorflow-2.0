
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href=../../../../default.css" rel="stylesheet">
    <link href="
   ../../../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.keras.layers.AdditiveAttention" />
<meta itemprop="path" content="Stable" />
<meta itemprop="property" content="__init__"/>
</div>

<h1 id="tfkeraslayersadditiveattention">tf.keras.layers.AdditiveAttention</h1>
<!-- Insert buttons -->

<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p><a target="_blank" href="/code/stable/tensorflow/python/keras/layers/dense_attention.py">View source</a></p>
<h2 id="class-additiveattention">Class <code>AdditiveAttention</code></h2>
<!-- Start diff -->

<p>Additive attention layer, a.k.a. Bahdanau-style attention.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.keras.layers.AdditiveAttention</code></li>
<li>Class <code>tf.compat.v2.keras.layers.AdditiveAttention</code></li>
</ul>
<!-- Placeholder for "Used in" -->

<p>Inputs are <code>query</code> tensor of shape <code>[batch_size, Tq, dim]</code>, <code>value</code> tensor of
shape <code>[batch_size, Tv, dim]</code> and <code>key</code> tensor of shape
<code>[batch_size, Tv, dim]</code>. The calculation follows the steps:</p>
<ol>
<li>Reshape <code>query</code> and <code>value</code> into shapes <code>[batch_size, Tq, 1, dim]</code>
   and <code>[batch_size, 1, Tv, dim]</code> respectively.</li>
<li>Calculate scores with shape <code>[batch_size, Tq, Tv]</code> as a non-linear
   sum: <code>scores = tf.reduce_sum(tf.tanh(query + value), axis=-1)</code></li>
<li>Use scores to calculate a distribution with shape
   <code>[batch_size, Tq, Tv]</code>: <code>distribution = tf.nn.softmax(scores)</code>.</li>
<li>Use <code>distribution</code> to create a linear combination of <code>value</code> with
   shape <code>batch_size, Tq, dim]</code>:
   <code>return tf.matmul(distribution, value)</code>.</li>
</ol>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>use_scale</code></b>: If <code>True</code>, will create a variable to scale the attention scores.</li>
<li><b><code>causal</code></b>: Boolean. Set to <code>True</code> for decoder self-attention. Adds a mask such
  that position <code>i</code> cannot attend to positions <code>j &gt; i</code>. This prevents the
  flow of information from the future towards the past.</li>
</ul>
<h4 id="call-arguments">Call Arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: List of the following tensors:</li>
<li>query: Query <code>Tensor</code> of shape <code>[batch_size, Tq, dim]</code>.</li>
<li>value: Value <code>Tensor</code> of shape <code>[batch_size, Tv, dim]</code>.</li>
<li>key: Optional key <code>Tensor</code> of shape <code>[batch_size, Tv, dim]</code>. If not
    given, will use <code>value</code> for both <code>key</code> and <code>value</code>, which is the
    most common case.</li>
<li><b><code>mask</code></b>: List of the following tensors:</li>
<li>query_mask: A boolean mask <code>Tensor</code> of shape <code>[batch_size, Tq]</code>.
    If given, the output will be zero at the positions where
    <code>mask==False</code>.</li>
<li>value_mask: A boolean mask <code>Tensor</code> of shape <code>[batch_size, Tv]</code>.
    If given, will apply the mask such that values at positions where
    <code>mask==False</code> do not contribute to the result.</li>
</ul>
<h4 id="output-shape">Output shape:</h4>
<p>Attention outputs of shape <code>[batch_size, Tq, dim]</code>.</p>
<p>The meaning of <code>query</code>, <code>value</code> and <code>key</code> depend on the application. In the
case of text similarity, for example, <code>query</code> is the sequence embeddings of
the first piece of text and <code>value</code> is the sequence embeddings of the second
piece of text. <code>key</code> is usually the same tensor as <code>value</code>.</p>
<p>Here is a code example for using <code>AdditiveAttention</code> in a CNN+Attention
network:</p>
<div class="codehilite"><pre><span></span><span class="c1"># Variable-length int sequences.</span>
<span class="n">query_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">)</span>
<span class="n">value_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">)</span>

<span class="c1"># Embedding lookup.</span>
<span class="n">token_embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_tokens</span><span class="p">,</span> <span class="n">dimension</span><span class="p">)</span>
<span class="c1"># Query embeddings of shape [batch_size, Tq, dimension].</span>
<span class="n">query_embeddings</span> <span class="o">=</span> <span class="n">token_embedding</span><span class="p">(</span><span class="n">query_input</span><span class="p">)</span>
<span class="c1"># Value embeddings of shape [batch_size, Tv, dimension].</span>
<span class="n">value_embeddings</span> <span class="o">=</span> <span class="n">token_embedding</span><span class="p">(</span><span class="n">query_input</span><span class="p">)</span>

<span class="c1"># CNN layer.</span>
<span class="n">cnn_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv1D</span><span class="p">(</span>
    <span class="n">filters</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="c1"># Use &#39;same&#39; padding so outputs have the same shape as inputs.</span>
    <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)</span>
<span class="c1"># Query encoding of shape [batch_size, Tq, filters].</span>
<span class="n">query_seq_encoding</span> <span class="o">=</span> <span class="n">cnn_layer</span><span class="p">(</span><span class="n">query_embeddings</span><span class="p">)</span>
<span class="c1"># Value encoding of shape [batch_size, Tv, filters].</span>
<span class="n">value_seq_encoding</span> <span class="o">=</span> <span class="n">cnn_layer</span><span class="p">(</span><span class="n">value_embeddings</span><span class="p">)</span>

<span class="c1"># Query-value attention of shape [batch_size, Tq, filters].</span>
<span class="n">query_value_attention_seq</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">AdditiveAttention</span><span class="p">()(</span>
    <span class="p">[</span><span class="n">query_seq_encoding</span><span class="p">,</span> <span class="n">value_seq_encoding</span><span class="p">])</span>

<span class="c1"># Reduce over the sequence axis to produce encodings of shape</span>
<span class="c1"># [batch_size, filters].</span>
<span class="n">query_encoding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling1D</span><span class="p">()(</span>
    <span class="n">query_seq_encoding</span><span class="p">)</span>
<span class="n">query_value_attention</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling1D</span><span class="p">()(</span>
    <span class="n">query_value_attention_seq</span><span class="p">)</span>

<span class="c1"># Concatenate query and document encodings to produce a DNN input layer.</span>
<span class="n">input_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">()(</span>
    <span class="p">[</span><span class="n">query_encoding</span><span class="p">,</span> <span class="n">query_value_attention</span><span class="p">])</span>

<span class="c1"># Add DNN layers, and create Model.</span>
<span class="c1"># ...</span>
</pre></div>


<h2 id="__init__"><code>__init__</code></h2>

<p><a target="_blank" href="/code/stable/tensorflow/python/keras/layers/dense_attention.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="fm">__init__</span><span class="p">(</span>
    <span class="n">use_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>
    </body>
    </html>
   