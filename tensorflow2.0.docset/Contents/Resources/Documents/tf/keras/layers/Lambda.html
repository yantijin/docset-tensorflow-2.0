
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href="../../../../default.css" rel="stylesheet">
    <link href="
   ../../../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.keras.layers.Lambda" />
<meta itemprop="path" content="Stable" />
<meta itemprop="property" content="__init__"/>
</div>

<h1 id="tfkeraslayerslambda">tf.keras.layers.Lambda</h1>
<!-- Insert buttons -->

<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/core.py">View source</a></p>
<h2 id="class-lambda">Class <code>Lambda</code></h2>
<!-- Start diff -->

<p>Wraps arbitrary expressions as a <code>Layer</code> object.</p>
<p>Inherits From: <a href="../../../tf/keras/layers/Layer.html"><code>Layer</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.keras.layers.Lambda</code></li>
<li>Class <code>tf.compat.v2.keras.layers.Lambda</code></li>
</ul>
<!-- Placeholder for "Used in" -->

<p>The <code>Lambda</code> layer exists so that arbitrary TensorFlow functions
can be used when constructing <code>Sequential</code> and Functional API
models. <code>Lambda</code> layers are best suited for simple operations or
quick experimentation. For more advanced use cases, subclassing
<a href="../../../tf/keras/layers/Layer.html"><code>keras.layers.Layer</code></a> is preferred. One reason for this is that
when saving a Model, <code>Lambda</code> layers are saved by serializing the
Python bytecode, whereas subclassed Layers are saved via overriding
their <code>get_config</code> method and are thus more portable. Models that rely
on subclassed Layers are also often easier to visualize and reason
about.</p>
<h4 id="examples">Examples:</h4>
<div class="codehilite"><pre><span></span><span class="c1"># add a x -&gt; x^2 layer</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>


<div class="codehilite"><pre><span></span><span class="c1"># add a layer that returns the concatenation</span>
<span class="c1"># of the positive part of the input and</span>
<span class="c1"># the opposite of the negative part</span>

<span class="k">def</span> <span class="nf">antirectifier</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">-=</span> <span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">l2_normalize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">neg</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">pos</span><span class="p">,</span> <span class="n">neg</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Lambda</span><span class="p">(</span><span class="n">antirectifier</span><span class="p">))</span>
</pre></div>


<p>Variables can be created within a <code>Lambda</code> layer. Like with
other layers, these variables will be created only once and reused
if the <code>Lambda</code> layer is called on new inputs. If creating more
than one variable in a given <code>Lambda</code> instance, be sure to use
a different name for each variable. Note that calling sublayers
from within a <code>Lambda</code> is not supported.</p>
<p>Example of variable creation:</p>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">linear_transform</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="n">v1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;multiplier&#39;</span><span class="p">)</span>
  <span class="n">v2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;bias&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="n">v1</span> <span class="o">+</span> <span class="n">v2</span>

<span class="n">linear_layer</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="n">linear_transform</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">linear_layer</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">linear_layer</span><span class="p">)</span>  <span class="c1"># Reuses existing Variables</span>
</pre></div>


<p>Note that creating two instances of <code>Lambda</code> using the same function
will <em>not</em> share Variables between the two instances. Each instance of
<code>Lambda</code> will create and manage its own weights.</p>
<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>function</code></b>: The function to be evaluated. Takes input tensor as first
  argument.</li>
<li><b><code>output_shape</code></b>: Expected output shape from function. This argument can be
  inferred if not explicitly provided. Can be a tuple or function. If a
  tuple, it only specifies the first dimension onward;
  sample dimension is assumed either the same as the input: <code>output_shape =
    (input_shape[0], ) + output_shape</code> or, the input is <code>None</code> and
  the sample dimension is also <code>None</code>: <code>output_shape = (None, ) +
    output_shape</code> If a function, it specifies the entire shape as a function
    of the
  input shape: <code>output_shape = f(input_shape)</code></li>
<li><b><code>mask</code></b>: Either None (indicating no masking) or a callable with the same
  signature as the <code>compute_mask</code> layer method, or a tensor that will be
  returned as output mask regardless what the input is.</li>
<li><b><code>arguments</code></b>: Optional dictionary of keyword arguments to be passed to the
  function.
Input shape: Arbitrary. Use the keyword argument input_shape (tuple of
  integers, does not include the samples axis) when using this layer as the
  first layer in a model.
Output shape: Specified by <code>output_shape</code> argument</li>
</ul>
<h2 id="__init__"><code>__init__</code></h2>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/core.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="fm">__init__</span><span class="p">(</span>
    <span class="n">function</span><span class="p">,</span>
    <span class="n">output_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">arguments</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>
    </body>
    </html>
   