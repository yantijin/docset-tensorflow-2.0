<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.keras.layers.Lambda" />
<meta itemprop="path" content="Stable" />
<meta itemprop="property" content="__init__"/>
</div>


<h1>tf.keras.layers.Lambda</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/layers/core.py">View source</a></p>

<h2>Class <code>Lambda</code></h2>

<!-- Start diff -->


<p>Wraps arbitrary expressions as a <code>Layer</code> object.</p>

<p>Inherits From: <a href="../../../tf/keras/layers/Layer.html"><code>Layer</code></a></p>

<h3>Aliases:</h3>

<ul>
<li>Class <code>tf.compat.v1.keras.layers.Lambda</code></li>
<li>Class <code>tf.compat.v2.keras.layers.Lambda</code></li>
</ul>


<!-- Placeholder for "Used in" -->


<p>The <code>Lambda</code> layer exists so that arbitrary TensorFlow functions
can be used when constructing <code>Sequential</code> and Functional API
models. <code>Lambda</code> layers are best suited for simple operations or
quick experimentation. For more advanced use cases, subclassing
<a href="../../../tf/keras/layers/Layer.html"><code>keras.layers.Layer</code></a> is preferred. One reason for this is that
when saving a Model, <code>Lambda</code> layers are saved by serializing the
Python bytecode, whereas subclassed Layers are saved via overriding
their <code>get_config</code> method and are thus more portable. Models that rely
on subclassed Layers are also often easier to visualize and reason
about.</p>

<h4>Examples:</h4>

<p>```python</p>

<h1>add a x -> x<sup>2</sup> layer</h1>

<p>model.add(Lambda(lambda x: x ** 2))
<code>
</code>python</p>

<h1>add a layer that returns the concatenation</h1>

<h1>of the positive part of the input and</h1>

<h1>the opposite of the negative part</h1>

<p>def antirectifier(x):
    x -= K.mean(x, axis=1, keepdims=True)
    x = K.l2_normalize(x, axis=1)
    pos = K.relu(x)
    neg = K.relu(-x)
    return K.concatenate([pos, neg], axis=1)</p>

<p>model.add(Lambda(antirectifier))
```</p>

<p>Variables can be created within a <code>Lambda</code> layer. Like with
other layers, these variables will be created only once and reused
if the <code>Lambda</code> layer is called on new inputs. If creating more
than one variable in a given <code>Lambda</code> instance, be sure to use
a different name for each variable. Note that calling sublayers
from within a <code>Lambda</code> is not supported.</p>

<p>Example of variable creation:</p>

<p>```python
def linear_transform(x):
  v1 = tf.Variable(1., name=&lsquo;multiplier&rsquo;)
  v2 = tf.Variable(0., name=&lsquo;bias&rsquo;)
  return x*v1 + v2</p>

<p>linear_layer = Lambda(linear_transform)
model.add(linear_layer)
model.add(keras.layers.Dense(10, activation=&lsquo;relu&rsquo;))
model.add(linear_layer)  # Reuses existing Variables
```</p>

<p>Note that creating two instances of <code>Lambda</code> using the same function
will <em>not</em> share Variables between the two instances. Each instance of
<code>Lambda</code> will create and manage its own weights.</p>

<h4>Arguments:</h4>

<ul>
<li><b><code>function</code></b>: The function to be evaluated. Takes input tensor as first
argument.</li>
<li><b><code>output_shape</code></b>: Expected output shape from function. This argument can be
inferred if not explicitly provided. Can be a tuple or function. If a
tuple, it only specifies the first dimension onward;
sample dimension is assumed either the same as the input: <code>output_shape =
  (input_shape[0], ) + output_shape</code> or, the input is <code>None</code> and
the sample dimension is also <code>None</code>: <code>output_shape = (None, ) +
  output_shape</code> If a function, it specifies the entire shape as a function
  of the
input shape: <code>output_shape = f(input_shape)</code></li>
<li><b><code>mask</code></b>: Either None (indicating no masking) or a callable with the same
signature as the <code>compute_mask</code> layer method, or a tensor that will be
returned as output mask regardless what the input is.</li>
<li><b><code>arguments</code></b>: Optional dictionary of keyword arguments to be passed to the
function.
Input shape: Arbitrary. Use the keyword argument input_shape (tuple of
integers, does not include the samples axis) when using this layer as the
first layer in a model.
Output shape: Specified by <code>output_shape</code> argument</li>
</ul>


<h2 id="__init__"><code>__init__</code></h2>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/layers/core.py">View source</a></p>

<p><code>python
__init__(
    function,
    output_shape=None,
    mask=None,
    arguments=None,
    **kwargs
)
</code></p>
