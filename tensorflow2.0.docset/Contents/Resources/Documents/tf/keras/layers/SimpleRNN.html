
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href="../../../../default.css" rel="stylesheet">
    <link href="
   ../../../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.keras.layers.SimpleRNN" />
<meta itemprop="path" content="Stable" />
<meta itemprop="property" content="activation"/>
<meta itemprop="property" content="bias_constraint"/>
<meta itemprop="property" content="bias_initializer"/>
<meta itemprop="property" content="bias_regularizer"/>
<meta itemprop="property" content="dropout"/>
<meta itemprop="property" content="kernel_constraint"/>
<meta itemprop="property" content="kernel_initializer"/>
<meta itemprop="property" content="kernel_regularizer"/>
<meta itemprop="property" content="recurrent_constraint"/>
<meta itemprop="property" content="recurrent_dropout"/>
<meta itemprop="property" content="recurrent_initializer"/>
<meta itemprop="property" content="recurrent_regularizer"/>
<meta itemprop="property" content="states"/>
<meta itemprop="property" content="units"/>
<meta itemprop="property" content="use_bias"/>
<meta itemprop="property" content="__init__"/>
<meta itemprop="property" content="get_initial_state"/>
<meta itemprop="property" content="reset_states"/>
</div>

<h1 id="tfkeraslayerssimplernn">tf.keras.layers.SimpleRNN</h1>
<!-- Insert buttons -->

<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/recurrent.py">View source</a></p>
<h2 id="class-simplernn">Class <code>SimpleRNN</code></h2>
<!-- Start diff -->

<p>Fully-connected RNN where the output is to be fed back to input.</p>
<p>Inherits From: <a href="../../../tf/keras/layers/RNN.html"><code>RNN</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.keras.layers.SimpleRNN</code></li>
<li>Class <code>tf.compat.v2.keras.layers.SimpleRNN</code></li>
</ul>
<!-- Placeholder for "Used in" -->

<h4 id="arguments">Arguments:</h4>
<ul>
<li><b><code>units</code></b>: Positive integer, dimensionality of the output space.</li>
<li><b><code>activation</code></b>: Activation function to use.
  Default: hyperbolic tangent (<code>tanh</code>).
  If you pass None, no activation is applied
  (ie. "linear" activation: <code>a(x) = x</code>).</li>
<li><b><code>use_bias</code></b>: Boolean, whether the layer uses a bias vector.</li>
<li><b><code>kernel_initializer</code></b>: Initializer for the <code>kernel</code> weights matrix,
  used for the linear transformation of the inputs.</li>
<li><b><code>recurrent_initializer</code></b>: Initializer for the <code>recurrent_kernel</code>
  weights matrix,
  used for the linear transformation of the recurrent state.</li>
<li><b><code>bias_initializer</code></b>: Initializer for the bias vector.</li>
<li><b><code>kernel_regularizer</code></b>: Regularizer function applied to
  the <code>kernel</code> weights matrix.</li>
<li><b><code>recurrent_regularizer</code></b>: Regularizer function applied to
  the <code>recurrent_kernel</code> weights matrix.</li>
<li><b><code>bias_regularizer</code></b>: Regularizer function applied to the bias vector.</li>
<li><b><code>activity_regularizer</code></b>: Regularizer function applied to
  the output of the layer (its "activation")..</li>
<li><b><code>kernel_constraint</code></b>: Constraint function applied to
  the <code>kernel</code> weights matrix.</li>
<li><b><code>recurrent_constraint</code></b>: Constraint function applied to
  the <code>recurrent_kernel</code> weights matrix.</li>
<li><b><code>bias_constraint</code></b>: Constraint function applied to the bias vector.</li>
<li><b><code>dropout</code></b>: Float between 0 and 1.
  Fraction of the units to drop for
  the linear transformation of the inputs.</li>
<li><b><code>recurrent_dropout</code></b>: Float between 0 and 1.
  Fraction of the units to drop for
  the linear transformation of the recurrent state.</li>
<li><b><code>return_sequences</code></b>: Boolean. Whether to return the last output
  in the output sequence, or the full sequence.</li>
<li><b><code>return_state</code></b>: Boolean. Whether to return the last state
  in addition to the output.</li>
<li><b><code>go_backwards</code></b>: Boolean (default False).
  If True, process the input sequence backwards and return the
  reversed sequence.</li>
<li><b><code>stateful</code></b>: Boolean (default False). If True, the last state
  for each sample at index i in a batch will be used as initial
  state for the sample of index i in the following batch.</li>
<li><b><code>unroll</code></b>: Boolean (default False).
  If True, the network will be unrolled,
  else a symbolic loop will be used.
  Unrolling can speed-up a RNN,
  although it tends to be more memory-intensive.
  Unrolling is only suitable for short sequences.</li>
</ul>
<h4 id="call-arguments">Call arguments:</h4>
<ul>
<li><b><code>inputs</code></b>: A 3D tensor.</li>
<li><b><code>mask</code></b>: Binary tensor of shape <code>(samples, timesteps)</code> indicating whether
  a given timestep should be masked.</li>
<li><b><code>training</code></b>: Python boolean indicating whether the layer should behave in
  training mode or in inference mode. This argument is passed to the cell
  when calling it. This is only relevant if <code>dropout</code> or
  <code>recurrent_dropout</code> is used.</li>
<li><b><code>initial_state</code></b>: List of initial state tensors to be passed to the first
  call of the cell.</li>
</ul>
<h2 id="__init__"><code>__init__</code></h2>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/recurrent.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="fm">__init__</span><span class="p">(</span>
    <span class="n">units</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;glorot_uniform&#39;</span><span class="p">,</span>
    <span class="n">recurrent_initializer</span><span class="o">=</span><span class="s1">&#39;orthogonal&#39;</span><span class="p">,</span>
    <span class="n">bias_initializer</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
    <span class="n">kernel_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">recurrent_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bias_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">activity_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">recurrent_constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bias_constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">return_state</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">go_backwards</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">stateful</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">unroll</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>


<h2 id="properties">Properties</h2>
<h3 id="activation"><code>activation</code></h3>

<h3 id="bias_constraint"><code>bias_constraint</code></h3>

<h3 id="bias_initializer"><code>bias_initializer</code></h3>

<h3 id="bias_regularizer"><code>bias_regularizer</code></h3>

<h3 id="dropout"><code>dropout</code></h3>

<h3 id="kernel_constraint"><code>kernel_constraint</code></h3>

<h3 id="kernel_initializer"><code>kernel_initializer</code></h3>

<h3 id="kernel_regularizer"><code>kernel_regularizer</code></h3>

<h3 id="recurrent_constraint"><code>recurrent_constraint</code></h3>

<h3 id="recurrent_dropout"><code>recurrent_dropout</code></h3>

<h3 id="recurrent_initializer"><code>recurrent_initializer</code></h3>

<h3 id="recurrent_regularizer"><code>recurrent_regularizer</code></h3>

<h3 id="states"><code>states</code></h3>

<h3 id="units"><code>units</code></h3>

<h3 id="use_bias"><code>use_bias</code></h3>

<h2 id="methods">Methods</h2>
<h3 id="get_initial_state"><code>get_initial_state</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/recurrent.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">get_initial_state</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>


<h3 id="reset_states"><code>reset_states</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/recurrent.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">reset_states</span><span class="p">(</span><span class="n">states</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
    </body>
    </html>
   