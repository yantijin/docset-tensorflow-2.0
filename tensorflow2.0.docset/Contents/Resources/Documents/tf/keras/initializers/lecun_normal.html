<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.keras.initializers.lecun_normal" />
<meta itemprop="path" content="Stable" />
</div>


<h1>tf.keras.initializers.lecun_normal</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/ops/init_ops_v2.py">View source</a></p>

<!-- Start diff -->


<p>LeCun normal initializer.</p>

<h3>Aliases:</h3>

<ul>
<li><code>tf.compat.v2.initializers.lecun_normal</code></li>
<li><code>tf.compat.v2.keras.initializers.lecun_normal</code></li>
<li><code>tf.initializers.lecun_normal</code></li>
</ul>


<p><code>python
tf.keras.initializers.lecun_normal(seed=None)
</code></p>

<!-- Placeholder for "Used in" -->


<p>It draws samples from a truncated normal distribution centered on 0
with <code>stddev = sqrt(1 / fan_in)</code>
where <code>fan_in</code> is the number of input units in the weight tensor.</p>

<h4>Arguments:</h4>

<ul>
<li><b><code>seed</code></b>: A Python integer. Used to seed the random generator.</li>
</ul>


<h4>Returns:</h4>

<p>An initializer.</p>

<h4>References:</h4>

<ul>
<li>Self-Normalizing Neural Networks,
[Klambauer et al., 2017]
(https://papers.nips.cc/paper/6698-self-normalizing-neural-networks)
([pdf]
(https://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf))</li>
<li>Efficient Backprop,
<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Lecun et al., 1998</a></li>
</ul>

