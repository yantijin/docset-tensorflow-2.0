<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.keras.utils.multi_gpu_model" />
<meta itemprop="path" content="Stable" />
</div>


<h1>tf.keras.utils.multi_gpu_model</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/keras/utils/multi_gpu_utils.py">View source</a></p>

<!-- Start diff -->


<p>Replicates a model on different GPUs. (deprecated)</p>

<h3>Aliases:</h3>

<ul>
<li><code>tf.compat.v1.keras.utils.multi_gpu_model</code></li>
<li><code>tf.compat.v2.keras.utils.multi_gpu_model</code></li>
</ul>


<p><code>python
tf.keras.utils.multi_gpu_model(
    model,
    gpus,
    cpu_merge=True,
    cpu_relocation=False
)
</code></p>

<!-- Placeholder for "Used in" -->


<p>Warning: THIS FUNCTION IS DEPRECATED. It will be removed after 2020-04-01.
Instructions for updating:
Use <a href="../../../tf/distribute/MirroredStrategy.html"><code>tf.distribute.MirroredStrategy</code></a> instead.</p>

<p>Specifically, this function implements single-machine
multi-GPU data parallelism. It works in the following way:</p>

<ul>
<li>Divide the model&rsquo;s input(s) into multiple sub-batches.</li>
<li>Apply a model copy on each sub-batch. Every model copy
  is executed on a dedicated GPU.</li>
<li>Concatenate the results (on CPU) into one big batch.</li>
</ul>


<p>E.g. if your <code>batch_size</code> is 64 and you use <code>gpus=2</code>,
then we will divide the input into 2 sub-batches of 32 samples,
process each sub-batch on one GPU, then return the full
batch of 64 processed samples.</p>

<p>This induces quasi-linear speedup on up to 8 GPUs.</p>

<p>This function is only available with the TensorFlow backend
for the time being.</p>

<h4>Arguments:</h4>

<ul>
<li><b><code>model</code></b>: A Keras model instance. To avoid OOM errors,
  this model could have been built on CPU, for instance
  (see usage example below).</li>
<li><b><code>gpus</code></b>: Integer >= 2, number of on GPUs on which to create
  model replicas.</li>
<li><b><code>cpu_merge</code></b>: A boolean value to identify whether to force
  merging model weights under the scope of the CPU or not.</li>
<li><b><code>cpu_relocation</code></b>: A boolean value to identify whether to
  create the model&rsquo;s weights under the scope of the CPU.
  If the model is not defined under any preceding device
  scope, you can still rescue it by activating this option.</li>
</ul>


<h4>Returns:</h4>

<p>A Keras <code>Model</code> instance which can be used just like the initial
<code>model</code> argument, but which distributes its workload on multiple GPUs.</p>

<p>Example 1: Training models with weights merge on CPU</p>

<p>```python
    import tensorflow as tf
    from keras.applications import Xception
    from keras.utils import multi_gpu_model
    import numpy as np</p>

<pre><code>num_samples = 1000
height = 224
width = 224
num_classes = 1000

# Instantiate the base model (or "template" model).
# We recommend doing this with under a CPU device scope,
# so that the model's weights are hosted on CPU memory.
# Otherwise they may end up hosted on a GPU, which would
# complicate weight sharing.
with tf.device('/cpu:0'):
    model = Xception(weights=None,
                     input_shape=(height, width, 3),
                     classes=num_classes)

# Replicates the model on 8 GPUs.
# This assumes that your machine has 8 available GPUs.
parallel_model = multi_gpu_model(model, gpus=8)
parallel_model.compile(loss='categorical_crossentropy',
                       optimizer='rmsprop')

# Generate dummy data.
x = np.random.random((num_samples, height, width, 3))
y = np.random.random((num_samples, num_classes))

# This `fit` call will be distributed on 8 GPUs.
# Since the batch size is 256, each GPU will process 32 samples.
parallel_model.fit(x, y, epochs=20, batch_size=256)

# Save model via the template model (which shares the same weights):
model.save('my_model.h5')
</code></pre>

<p>```</p>

<p>Example 2: Training models with weights merge on CPU using cpu_relocation</p>

<p>```python
     ..
     # Not needed to change the device scope for model definition:
     model = Xception(weights=None, ..)</p>

<pre><code> try:
     model = multi_gpu_model(model, cpu_relocation=True)
     print("Training using multiple GPUs..")
 except:
     print("Training using single GPU or CPU..")

 model.compile(..)
 ..
</code></pre>

<p>```</p>

<p>Example 3: Training models with weights merge on GPU (recommended for NV-link)</p>

<p>```python
     ..
     # Not needed to change the device scope for model definition:
     model = Xception(weights=None, ..)</p>

<pre><code> try:
     model = multi_gpu_model(model, cpu_merge=False)
     print("Training using multiple GPUs..")
 except:
     print("Training using single GPU or CPU..")
 model.compile(..)
 ..
</code></pre>

<p>```</p>

<h4>Raises:</h4>

<ul>
<li><b><code>ValueError</code></b>: if the <code>gpus</code> argument does not match available devices.</li>
</ul>

