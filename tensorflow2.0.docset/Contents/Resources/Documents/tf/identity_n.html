<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.identity_n" />
<meta itemprop="path" content="Stable" />
</div>


<h1>tf.identity_n</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p>Defined in generated file: <code>python/ops/gen_array_ops.py</code></p>

<!-- Start diff -->


<p>Returns a list of tensors with the same shapes and contents as the input</p>

<h3>Aliases:</h3>

<ul>
<li><code>tf.compat.v1.identity_n</code></li>
<li><code>tf.compat.v2.identity_n</code></li>
</ul>


<p><code>python
tf.identity_n(
    input,
    name=None
)
</code></p>

<!-- Placeholder for "Used in" -->


<p>tensors.</p>

<p>This op can be used to override the gradient for complicated functions. For
example, suppose y = f(x) and we wish to apply a custom function g for backprop
such that dx = g(dy). In Python,</p>

<p>```python
with tf.get_default_graph().gradient_override_map(
    {&lsquo;IdentityN&rsquo;: &lsquo;OverrideGradientWithG&rsquo;}):
  y, _ = identity_n([f(x), x])</p>

<p>@tf.RegisterGradient(&lsquo;OverrideGradientWithG&rsquo;)
def ApplyG(op, dy, _):
  return [None, g(dy)]  # Do not backprop to f(x).
```</p>

<h4>Args:</h4>

<ul>
<li><b><code>input</code></b>: A list of <code>Tensor</code> objects.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>


<h4>Returns:</h4>

<p>A list of <code>Tensor</code> objects. Has the same type as <code>input</code>.</p>
