<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.data.experimental.parallel_interleave" />
<meta itemprop="path" content="Stable" />
</div>


<h1>tf.data.experimental.parallel_interleave</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/data/experimental/ops/interleave_ops.py">View source</a></p>

<!-- Start diff -->


<p>A parallel version of the <a href="../../../tf/data/Dataset.html#interleave"><code>Dataset.interleave()</code></a> transformation. (deprecated)</p>

<h3>Aliases:</h3>

<ul>
<li><code>tf.compat.v1.data.experimental.parallel_interleave</code></li>
<li><code>tf.compat.v2.data.experimental.parallel_interleave</code></li>
</ul>


<p><code>python
tf.data.experimental.parallel_interleave(
    map_func,
    cycle_length,
    block_length=1,
    sloppy=False,
    buffer_output_elements=None,
    prefetch_input_elements=None
)
</code></p>

<!-- Placeholder for "Used in" -->


<p>Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="../../../tf/data/Dataset.html#interleave"><code>tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)</code></a> instead. If sloppy execution is desired, use <code>tf.data.Options.experimental_determinstic</code>.</p>

<p><code>parallel_interleave()</code> maps <code>map_func</code> across its input to produce nested
datasets, and outputs their elements interleaved. Unlike
<a href="../../../tf/data/Dataset.html#interleave"><code>tf.data.Dataset.interleave</code></a>, it gets elements from <code>cycle_length</code> nested
datasets in parallel, which increases the throughput, especially in the
presence of stragglers. Furthermore, the <code>sloppy</code> argument can be used to
improve performance, by relaxing the requirement that the outputs are produced
in a deterministic order, and allowing the implementation to skip over nested
datasets whose elements are not readily available when requested.</p>

<h4>Example usage:</h4>

<p>```python</p>

<h1>Preprocess 4 files concurrently.</h1>

<p>filenames = tf.data.Dataset.list_files(&ldquo;/path/to/data/train*.tfrecords&rdquo;)
dataset = filenames.apply(
    tf.data.experimental.parallel_interleave(
        lambda filename: tf.data.TFRecordDataset(filename),
        cycle_length=4))
```</p>

<p>WARNING: If <code>sloppy</code> is <code>True</code>, the order of produced elements is not
deterministic.</p>

<h4>Args:</h4>

<ul>
<li><b><code>map_func</code></b>: A function mapping a nested structure of tensors to a <code>Dataset</code>.</li>
<li><b><code>cycle_length</code></b>: The number of input <code>Dataset</code>s to interleave from in parallel.</li>
<li><b><code>block_length</code></b>: The number of consecutive elements to pull from an input
<code>Dataset</code> before advancing to the next input <code>Dataset</code>.</li>
<li><b><code>sloppy</code></b>: If false, elements are produced in deterministic order. Otherwise,
the implementation is allowed, for the sake of expediency, to produce
elements in a non-deterministic order.</li>
<li><b><code>buffer_output_elements</code></b>: The number of elements each iterator being
interleaved should buffer (similar to the <code>.prefetch()</code> transformation for
each interleaved iterator).</li>
<li><b><code>prefetch_input_elements</code></b>: The number of input elements to transform to
iterators before they are needed for interleaving.</li>
</ul>


<h4>Returns:</h4>

<p>A <code>Dataset</code> transformation function, which can be passed to
<a href="../../../tf/data/Dataset.html#apply"><code>tf.data.Dataset.apply</code></a>.</p>
