<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.clip_by_global_norm" />
<meta itemprop="path" content="Stable" />
</div>


<h1>tf.clip_by_global_norm</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/ops/clip_ops.py">View source</a></p>

<!-- Start diff -->


<p>Clips values of multiple tensors by the ratio of the sum of their norms.</p>

<h3>Aliases:</h3>

<ul>
<li><code>tf.compat.v1.clip_by_global_norm</code></li>
<li><code>tf.compat.v2.clip_by_global_norm</code></li>
</ul>


<p><code>python
tf.clip_by_global_norm(
    t_list,
    clip_norm,
    use_norm=None,
    name=None
)
</code></p>

<!-- Placeholder for "Used in" -->


<p>Given a tuple or list of tensors <code>t_list</code>, and a clipping ratio <code>clip_norm</code>,
this operation returns a list of clipped tensors <code>list_clipped</code>
and the global norm (<code>global_norm</code>) of all tensors in <code>t_list</code>. Optionally,
if you&rsquo;ve already computed the global norm for <code>t_list</code>, you can specify
the global norm with <code>use_norm</code>.</p>

<p>To perform the clipping, the values <code>t_list[i]</code> are set to:</p>

<pre><code>t_list[i] * clip_norm / max(global_norm, clip_norm)
</code></pre>

<p>where:</p>

<pre><code>global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))
</code></pre>

<p>If <code>clip_norm &gt; global_norm</code> then the entries in <code>t_list</code> remain as they are,
otherwise they&rsquo;re all shrunk by the global ratio.</p>

<p>If <code>global_norm == infinity</code> then the entries in <code>t_list</code> are all set to <code>NaN</code>
to signal that an error occurred.</p>

<p>Any of the entries of <code>t_list</code> that are of type <code>None</code> are ignored.</p>

<p>This is the correct way to perform gradient clipping (for example, see
<a href="http://arxiv.org/abs/1211.5063">Pascanu et al., 2012</a>
(<a href="http://arxiv.org/pdf/1211.5063.pdf">pdf</a>)).</p>

<p>However, it is slower than <code>clip_by_norm()</code> because all the parameters must be
ready before the clipping operation can be performed.</p>

<h4>Args:</h4>

<ul>
<li><b><code>t_list</code></b>: A tuple or list of mixed <code>Tensors</code>, <code>IndexedSlices</code>, or None.</li>
<li><b><code>clip_norm</code></b>: A 0-D (scalar) <code>Tensor</code> > 0. The clipping ratio.</li>
<li><b><code>use_norm</code></b>: A 0-D (scalar) <code>Tensor</code> of type <code>float</code> (optional). The global
norm to use. If not provided, <code>global_norm()</code> is used to compute the norm.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>


<h4>Returns:</h4>

<ul>
<li><b><code>list_clipped</code></b>: A list of <code>Tensors</code> of the same type as <code>list_t</code>.</li>
<li><b><code>global_norm</code></b>: A 0-D (scalar) <code>Tensor</code> representing the global norm.</li>
</ul>


<h4>Raises:</h4>

<ul>
<li><b><code>TypeError</code></b>: If <code>t_list</code> is not a sequence.</li>
</ul>

