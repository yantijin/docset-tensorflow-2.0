
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href="../../default.css" rel="stylesheet">
    <link href="
   ../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.clip_by_global_norm" />
<meta itemprop="path" content="Stable" />
</div>

<h1 id="tfclip_by_global_norm">tf.clip_by_global_norm</h1>
<!-- Insert buttons -->

<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/clip_ops.py">View source</a></p>
<!-- Start diff -->

<p>Clips values of multiple tensors by the ratio of the sum of their norms.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v1.clip_by_global_norm</code></li>
<li><code>tf.compat.v2.clip_by_global_norm</code></li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span>
    <span class="n">t_list</span><span class="p">,</span>
    <span class="n">clip_norm</span><span class="p">,</span>
    <span class="n">use_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<!-- Placeholder for "Used in" -->

<p>Given a tuple or list of tensors <code>t_list</code>, and a clipping ratio <code>clip_norm</code>,
this operation returns a list of clipped tensors <code>list_clipped</code>
and the global norm (<code>global_norm</code>) of all tensors in <code>t_list</code>. Optionally,
if you've already computed the global norm for <code>t_list</code>, you can specify
the global norm with <code>use_norm</code>.</p>
<p>To perform the clipping, the values <code>t_list[i]</code> are set to:</p>
<div class="codehilite"><pre><span></span><span class="n">t_list</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">clip_norm</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="n">global_norm</span><span class="p">,</span><span class="w"> </span><span class="n">clip_norm</span><span class="p">)</span><span class="w"></span>
</pre></div>


<p>where:</p>
<div class="codehilite"><pre><span></span><span class="err">global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))</span>
</pre></div>


<p>If <code>clip_norm &gt; global_norm</code> then the entries in <code>t_list</code> remain as they are,
otherwise they're all shrunk by the global ratio.</p>
<p>If <code>global_norm == infinity</code> then the entries in <code>t_list</code> are all set to <code>NaN</code>
to signal that an error occurred.</p>
<p>Any of the entries of <code>t_list</code> that are of type <code>None</code> are ignored.</p>
<p>This is the correct way to perform gradient clipping (for example, see
<a href="http://arxiv.org/abs/1211.5063">Pascanu et al., 2012</a>
(<a href="http://arxiv.org/pdf/1211.5063.pdf">pdf</a>)).</p>
<p>However, it is slower than <code>clip_by_norm()</code> because all the parameters must be
ready before the clipping operation can be performed.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>t_list</code></b>: A tuple or list of mixed <code>Tensors</code>, <code>IndexedSlices</code>, or None.</li>
<li><b><code>clip_norm</code></b>: A 0-D (scalar) <code>Tensor</code> &gt; 0. The clipping ratio.</li>
<li><b><code>use_norm</code></b>: A 0-D (scalar) <code>Tensor</code> of type <code>float</code> (optional). The global
  norm to use. If not provided, <code>global_norm()</code> is used to compute the norm.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>
<h4 id="returns">Returns:</h4>
<ul>
<li><b><code>list_clipped</code></b>: A list of <code>Tensors</code> of the same type as <code>list_t</code>.</li>
<li><b><code>global_norm</code></b>: A 0-D (scalar) <code>Tensor</code> representing the global norm.</li>
</ul>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If <code>t_list</code> is not a sequence.</li>
</ul>
    </body>
    </html>
   