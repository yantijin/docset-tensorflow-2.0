<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.math.cumulative_logsumexp" />
<meta itemprop="path" content="Stable" />
</div>


<h1>tf.math.cumulative_logsumexp</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/ops/math_ops.py">View source</a></p>

<!-- Start diff -->


<p>Compute the cumulative log-sum-exp of the tensor <code>x</code> along <code>axis</code>.</p>

<h3>Aliases:</h3>

<ul>
<li><code>tf.compat.v1.math.cumulative_logsumexp</code></li>
<li><code>tf.compat.v2.math.cumulative_logsumexp</code></li>
</ul>


<p><code>python
tf.math.cumulative_logsumexp(
    x,
    axis=0,
    exclusive=False,
    reverse=False,
    name=None
)
</code></p>

<!-- Placeholder for "Used in" -->


<p>By default, this op performs an inclusive cumulative log-sum-exp, which means
that the first element of the input is identical to the first element of
the output.</p>

<p>This operation is significantly more numerically stable than the equivalent
tensorflow operation <code>tf.math.log(tf.math.cumsum(tf.math.exp(x)))</code>, although
computes the same result given infinite numerical precision. However, note
that in some cases, it may be less stable than <a href="../../tf/math/reduce_logsumexp.html"><code>tf.math.reduce_logsumexp</code></a>
for a given element, as it applies the &ldquo;log-sum-exp trick&rdquo; in a different
way.</p>

<p>More precisely, where <a href="../../tf/math/reduce_logsumexp.html"><code>tf.math.reduce_logsumexp</code></a> uses the following trick:</p>

<p><code>
log(sum(exp(x))) == log(sum(exp(x - max(x)))) + max(x)
</code></p>

<p>it cannot be directly used here as there is no fast way of applying it
to each prefix <code>x[:i]</code>. Instead, this function implements a prefix
scan using pairwise log-add-exp, which is a commutative and associative
(up to floating point precision) operator:</p>

<p><code>
log_add_exp(x, y) = log(exp(x) + exp(y))
                  = log(1 + exp(min(x, y) - max(x, y))) + max(x, y)
</code></p>

<p>However, reducing using the above operator leads to a different computation
tree (logs are taken repeatedly instead of only at the end), and the maximum
is only computed pairwise instead of over the entire prefix. In general, this
leads to a different and slightly less precise computation.</p>

<h4>Args:</h4>

<ul>
<li><b><code>x</code></b>: A <code>Tensor</code>. Must be one of the following types: <code>float16</code>, <code>float32</code>,
<code>float64</code>.</li>
<li><b><code>axis</code></b>: A <code>Tensor</code> of type <code>int32</code> or <code>int64</code> (default: 0). Must be in the
range <code>[-rank(x), rank(x))</code>.</li>
<li><b><code>exclusive</code></b>: If <code>True</code>, perform exclusive cumulative log-sum-exp.</li>
<li><b><code>reverse</code></b>: If <code>True</code>, performs the cumulative log-sum-exp in the reverse
direction.</li>
<li><b><code>name</code></b>: A name for the operation (optional).</li>
</ul>


<h4>Returns:</h4>

<p>A <code>Tensor</code>. Has the same shape and type as <code>x</code>.</p>
