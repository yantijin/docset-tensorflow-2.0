<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.linalg.lstsq" />
<meta itemprop="path" content="Stable" />
</div>


<h1>tf.linalg.lstsq</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/ops/linalg_ops.py">View source</a></p>

<!-- Start diff -->


<p>Solves one or more linear least-squares problems.</p>

<h3>Aliases:</h3>

<ul>
<li><code>tf.compat.v1.linalg.lstsq</code></li>
<li><code>tf.compat.v1.matrix_solve_ls</code></li>
<li><code>tf.compat.v2.linalg.lstsq</code></li>
</ul>


<p><code>python
tf.linalg.lstsq(
    matrix,
    rhs,
    l2_regularizer=0.0,
    fast=True,
    name=None
)
</code></p>

<!-- Placeholder for "Used in" -->


<p><code>matrix</code> is a tensor of shape <code>[..., M, N]</code> whose inner-most 2 dimensions
form <code>M</code>-by-<code>N</code> matrices. Rhs is a tensor of shape <code>[..., M, K]</code> whose
inner-most 2 dimensions form <code>M</code>-by-<code>K</code> matrices.  The computed output is a
<code>Tensor</code> of shape <code>[..., N, K]</code> whose inner-most 2 dimensions form <code>M</code>-by-<code>K</code>
matrices that solve the equations
<code>matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]</code> in the least squares
sense.</p>

<p>Below we will use the following notation for each pair of matrix and
right-hand sides in the batch:</p>

<p><code>matrix</code>=\(A \in \Re^{m \times n}\),
<code>rhs</code>=\(B  \in \Re^{m \times k}\),
<code>output</code>=\(X  \in \Re^{n \times k}\),
<code>l2_regularizer</code>=\(\lambda\).</p>

<p>If <code>fast</code> is <code>True</code>, then the solution is computed by solving the normal
equations using Cholesky decomposition. Specifically, if \(m \ge n\) then
\(X = (A<sup>T</sup> A + \lambda I)^{-1} A<sup>T</sup> B\), which solves the least-squares
problem \(X = \mathrm{argmin}<em>{Z \in \Re^{n \times k}} ||A Z - B||</em>F<sup>2</sup> +
\lambda ||Z||<em>F<sup>2</sup>\). If \(m \lt n\) then <code>output</code> is computed as
\(X = A<sup>T</sup> (A A<sup>T</sup> + \lambda I)^{-1} B\), which (for \(\lambda = 0\)) is
the minimum-norm solution to the under-determined linear system, i.e.
\(X = \mathrm{argmin}</em>{Z \in \Re^{n \times k}} ||Z||<em>F<sup>2</sup> \), subject to
\(A Z = B\). Notice that the fast path is only numerically stable when
\(A\) is numerically full rank and has a condition number
\(\mathrm{cond}(A) \lt \frac{1}{\sqrt{\epsilon</em>{mach}}}\) or\(\lambda\)
is sufficiently large.</p>

<p>If <code>fast</code> is <code>False</code> an algorithm based on the numerically robust complete
orthogonal decomposition is used. This computes the minimum-norm
least-squares solution, even when \(A\) is rank deficient. This path is
typically 6-7 times slower than the fast path. If <code>fast</code> is <code>False</code> then
<code>l2_regularizer</code> is ignored.</p>

<h4>Args:</h4>

<ul>
<li><b><code>matrix</code></b>: <code>Tensor</code> of shape <code>[..., M, N]</code>.</li>
<li><b><code>rhs</code></b>: <code>Tensor</code> of shape <code>[..., M, K]</code>.</li>
<li><b><code>l2_regularizer</code></b>: 0-D <code>double</code> <code>Tensor</code>. Ignored if <code>fast=False</code>.</li>
<li><b><code>fast</code></b>: bool. Defaults to <code>True</code>.</li>
<li><b><code>name</code></b>: string, optional name of the operation.</li>
</ul>


<h4>Returns:</h4>

<ul>
<li><b><code>output</code></b>: <code>Tensor</code> of shape <code>[..., N, K]</code> whose inner-most 2 dimensions form
<code>M</code>-by-<code>K</code> matrices that solve the equations
<code>matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]</code> in the least
squares sense.</li>
</ul>


<h4>Raises:</h4>

<ul>
<li><b><code>NotImplementedError</code></b>: linalg.lstsq is currently disabled for complex128
and l2_regularizer != 0 due to poor accuracy.</li>
</ul>

