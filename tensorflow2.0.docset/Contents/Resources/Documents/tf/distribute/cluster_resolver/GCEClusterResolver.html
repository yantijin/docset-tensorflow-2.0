
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href=../../../../default.css" rel="stylesheet">
    <link href="
   ../../../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.distribute.cluster_resolver.GCEClusterResolver" />
<meta itemprop="path" content="Stable" />
<meta itemprop="property" content="environment"/>
<meta itemprop="property" content="rpc_layer"/>
<meta itemprop="property" content="task_id"/>
<meta itemprop="property" content="task_type"/>
<meta itemprop="property" content="__init__"/>
<meta itemprop="property" content="cluster_spec"/>
<meta itemprop="property" content="master"/>
<meta itemprop="property" content="num_accelerators"/>
</div>

<h1 id="tfdistributecluster_resolvergceclusterresolver">tf.distribute.cluster_resolver.GCEClusterResolver</h1>
<!-- Insert buttons -->

<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p><a target="_blank" href="/code/stable/tensorflow/python/distribute/cluster_resolver/gce_cluster_resolver.py">View source</a></p>
<h2 id="class-gceclusterresolver">Class <code>GCEClusterResolver</code></h2>
<!-- Start diff -->

<p>ClusterResolver for Google Compute Engine.</p>
<p>Inherits From: <a href="../../../tf/distribute/cluster_resolver/ClusterResolver.html"><code>ClusterResolver</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.distribute.cluster_resolver.GCEClusterResolver</code></li>
<li>Class <code>tf.compat.v2.distribute.cluster_resolver.GCEClusterResolver</code></li>
</ul>
<!-- Placeholder for "Used in" -->

<p>This is an implementation of cluster resolvers for the Google Compute Engine
instance group platform. By specifying a project, zone, and instance group,
this will retrieve the IP address of all the instances within the instance
group and return a ClusterResolver object suitable for use for distributed
TensorFlow.</p>
<h2 id="__init__"><code>__init__</code></h2>

<p><a target="_blank" href="/code/stable/tensorflow/python/distribute/cluster_resolver/gce_cluster_resolver.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="fm">__init__</span><span class="p">(</span>
    <span class="n">project</span><span class="p">,</span>
    <span class="n">zone</span><span class="p">,</span>
    <span class="n">instance_group</span><span class="p">,</span>
    <span class="n">port</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="s1">&#39;worker&#39;</span><span class="p">,</span>
    <span class="n">task_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">rpc_layer</span><span class="o">=</span><span class="s1">&#39;grpc&#39;</span><span class="p">,</span>
    <span class="n">credentials</span><span class="o">=</span><span class="s1">&#39;default&#39;</span><span class="p">,</span>
    <span class="n">service</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Creates a new GCEClusterResolver object.</p>
<p>This takes in a few parameters and creates a GCEClusterResolver project. It
will then use these parameters to query the GCE API for the IP addresses of
each instance in the instance group.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>project</code></b>: Name of the GCE project.</li>
<li><b><code>zone</code></b>: Zone of the GCE instance group.</li>
<li><b><code>instance_group</code></b>: Name of the GCE instance group.</li>
<li><b><code>port</code></b>: Port of the listening TensorFlow server (default: 8470)</li>
<li><b><code>task_type</code></b>: Name of the TensorFlow job this GCE instance group of VM
  instances belong to.</li>
<li><b><code>task_id</code></b>: The task index for this particular VM, within the GCE
  instance group. In particular, every single instance should be assigned
  a unique ordinal index within an instance group manually so that they
  can be distinguished from each other.</li>
<li><b><code>rpc_layer</code></b>: The RPC layer TensorFlow should use to communicate across
  instances.</li>
<li><b><code>credentials</code></b>: GCE Credentials. If nothing is specified, this defaults to
  GoogleCredentials.get_application_default().</li>
<li><b><code>service</code></b>: The GCE API object returned by the googleapiclient.discovery
  function. (Default: discovery.build('compute', 'v1')). If you specify a
  custom service object, then the credentials parameter will be ignored.</li>
</ul>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ImportError</code></b>: If the googleapiclient is not installed.</li>
</ul>
<h2 id="properties">Properties</h2>
<h3 id="environment"><code>environment</code></h3>

<p>Returns the current environment which TensorFlow is running in.</p>
<p>There are two possible return values, "google" (when TensorFlow is running
in a Google-internal environment) or an empty string (when TensorFlow is
running elsewhere).</p>
<p>If you are implementing a ClusterResolver that works in both the Google
environment and the open-source world (for instance, a TPU ClusterResolver
or similar), you will have to return the appropriate string depending on the
environment, which you will have to detect.</p>
<p>Otherwise, if you are implementing a ClusterResolver that will only work
in open-source TensorFlow, you do not need to implement this property.</p>
<h3 id="rpc_layer"><code>rpc_layer</code></h3>

<h3 id="task_id"><code>task_id</code></h3>

<h3 id="task_type"><code>task_type</code></h3>

<h2 id="methods">Methods</h2>
<h3 id="cluster_spec"><code>cluster_spec</code></h3>

<p><a target="_blank" href="/code/stable/tensorflow/python/distribute/cluster_resolver/gce_cluster_resolver.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">cluster_spec</span><span class="p">()</span>
</pre></div>


<p>Returns a ClusterSpec object based on the latest instance group info.</p>
<p>This returns a ClusterSpec object for use based on information from the
specified instance group. We will retrieve the information from the GCE APIs
every time this method is called.</p>
<h4 id="returns">Returns:</h4>
<p>A ClusterSpec containing host information retrieved from GCE.</p>
<h3 id="master"><code>master</code></h3>

<p><a target="_blank" href="/code/stable/tensorflow/python/distribute/cluster_resolver/gce_cluster_resolver.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">master</span><span class="p">(</span>
    <span class="n">task_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">task_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">rpc_layer</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Retrieves the name or URL of the session master.</p>
<h4 id="args_1">Args:</h4>
<ul>
<li><b><code>task_type</code></b>: (Optional) The type of the TensorFlow task of the master.</li>
<li><b><code>task_id</code></b>: (Optional) The index of the TensorFlow task of the master.</li>
<li><b><code>rpc_layer</code></b>: (Optional) The RPC protocol for the given cluster.</li>
</ul>
<h4 id="returns_1">Returns:</h4>
<p>The name or URL of the session master.</p>
<p>Implementors of this function must take care in ensuring that the master
returned is up-to-date at the time to calling this function. This usually
means retrieving the master every time this function is invoked.</p>
<h3 id="num_accelerators"><code>num_accelerators</code></h3>

<p><a target="_blank" href="/code/stable/tensorflow/python/distribute/cluster_resolver/cluster_resolver.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">num_accelerators</span><span class="p">(</span>
    <span class="n">task_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">task_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">config_proto</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Returns the number of accelerator cores per worker.</p>
<p>This returns the number of accelerator cores (such as GPUs and TPUs)
available per worker.</p>
<p>Optionally, we allow callers to specify the task_type, and task_id, for
if they want to target a specific TensorFlow process to query
the number of accelerators. This is to support heterogenous environments,
where the number of accelerators cores per host is different.</p>
<h4 id="args_2">Args:</h4>
<ul>
<li><b><code>task_type</code></b>: (Optional) The type of the TensorFlow task of the machine we
  want to query.</li>
<li><b><code>task_id</code></b>: (Optional) The index of the TensorFlow task of the machine we
  want to query.</li>
<li><b><code>config_proto</code></b>: (Optional) Configuration for starting a new session to
  query how many accelerator cores it has.</li>
</ul>
<h4 id="returns_2">Returns:</h4>
<p>A map of accelerator types to number of cores.</p>
    </body>
    </html>
   