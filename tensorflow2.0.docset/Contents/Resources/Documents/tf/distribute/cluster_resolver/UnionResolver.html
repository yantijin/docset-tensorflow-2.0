
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href="../../../../default.css" rel="stylesheet">
    <link href="
   ../../../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.distribute.cluster_resolver.UnionResolver" />
<meta itemprop="path" content="Stable" />
<meta itemprop="property" content="environment"/>
<meta itemprop="property" content="rpc_layer"/>
<meta itemprop="property" content="task_id"/>
<meta itemprop="property" content="task_type"/>
<meta itemprop="property" content="__init__"/>
<meta itemprop="property" content="cluster_spec"/>
<meta itemprop="property" content="master"/>
<meta itemprop="property" content="num_accelerators"/>
</div>

<h1 id="tfdistributecluster_resolverunionresolver">tf.distribute.cluster_resolver.UnionResolver</h1>
<!-- Insert buttons -->

<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/distribute/cluster_resolver/cluster_resolver.py">View source</a></p>
<h2 id="class-unionresolver">Class <code>UnionResolver</code></h2>
<!-- Start diff -->

<p>Performs a union on underlying ClusterResolvers.</p>
<p>Inherits From: <a href="../../../tf/distribute/cluster_resolver/ClusterResolver.html"><code>ClusterResolver</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.distribute.cluster_resolver.UnionResolver</code></li>
<li>Class <code>tf.compat.v2.distribute.cluster_resolver.UnionResolver</code></li>
</ul>
<!-- Placeholder for "Used in" -->

<p>This class performs a union given two or more existing ClusterResolvers. It
merges the underlying ClusterResolvers, and returns one unified ClusterSpec
when cluster_spec is called. The details of the merge function is
documented in the cluster_spec function.</p>
<p>For additional ClusterResolver properties such as task type, task index,
rpc layer, environment, etc..., we will return the value from the first
ClusterResolver in the union.</p>
<h2 id="__init__"><code>__init__</code></h2>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/distribute/cluster_resolver/cluster_resolver.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="fm">__init__</span><span class="p">(</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>


<p>Initializes a UnionClusterResolver with other ClusterResolvers.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>*args</code></b>: <code>ClusterResolver</code> objects to be unionized.</li>
<li><b><code>**kwargs</code></b>:   rpc_layer - (Optional) Override value for the RPC layer used by
    TensorFlow.
  task_type - (Optional) Override value for the current task type.
  task_id - (Optional) Override value for the current task index.</li>
</ul>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>TypeError</code></b>: If any argument is not a subclass of <code>ClusterResolvers</code>.</li>
<li><b><code>ValueError</code></b>: If there are no arguments passed.</li>
</ul>
<h2 id="properties">Properties</h2>
<h3 id="environment"><code>environment</code></h3>

<p>Returns the current environment which TensorFlow is running in.</p>
<p>There are two possible return values, "google" (when TensorFlow is running
in a Google-internal environment) or an empty string (when TensorFlow is
running elsewhere).</p>
<p>If you are implementing a ClusterResolver that works in both the Google
environment and the open-source world (for instance, a TPU ClusterResolver
or similar), you will have to return the appropriate string depending on the
environment, which you will have to detect.</p>
<p>Otherwise, if you are implementing a ClusterResolver that will only work
in open-source TensorFlow, you do not need to implement this property.</p>
<h3 id="rpc_layer"><code>rpc_layer</code></h3>

<h3 id="task_id"><code>task_id</code></h3>

<h3 id="task_type"><code>task_type</code></h3>

<h2 id="methods">Methods</h2>
<h3 id="cluster_spec"><code>cluster_spec</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/distribute/cluster_resolver/cluster_resolver.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">cluster_spec</span><span class="p">()</span>
</pre></div>


<p>Returns a union of all the ClusterSpecs from the ClusterResolvers.</p>
<h4 id="returns">Returns:</h4>
<p>A ClusterSpec containing host information merged from all the underlying
ClusterResolvers.</p>
<h4 id="raises_1">Raises:</h4>
<ul>
<li><b><code>KeyError</code></b>: If there are conflicting keys detected when merging two or
more dictionaries, this exception is raised.</li>
</ul>
<p>Note: If there are multiple ClusterResolvers exposing ClusterSpecs with the
same job name, we will merge the list/dict of workers.</p>
<p>If <em>all</em> underlying ClusterSpecs expose the set of workers as lists, we will
concatenate the lists of workers, starting with the list of workers from
the first ClusterResolver passed into the constructor.</p>
<p>If <em>any</em> of the ClusterSpecs expose the set of workers as a dict, we will
treat all the sets of workers as dicts (even if they are returned as lists)
and will only merge them into a dict if there is no conflicting keys. If
there is a conflicting key, we will raise a <code>KeyError</code>.</p>
<h3 id="master"><code>master</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/distribute/cluster_resolver/cluster_resolver.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">master</span><span class="p">(</span>
    <span class="n">task_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">task_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">rpc_layer</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Returns the master address to use when creating a session.</p>
<p>This usually returns the master from the first ClusterResolver passed in,
but you can override this by specifying the task_type and task_id.</p>
<h4 id="args_1">Args:</h4>
<ul>
<li><b><code>task_type</code></b>: (Optional) The type of the TensorFlow task of the master.</li>
<li><b><code>task_id</code></b>: (Optional) The index of the TensorFlow task of the master.</li>
<li><b><code>rpc_layer</code></b>: (Optional) The RPC protocol for the given cluster.</li>
</ul>
<h4 id="returns_1">Returns:</h4>
<p>The name or URL of the session master.</p>
<h3 id="num_accelerators"><code>num_accelerators</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/distribute/cluster_resolver/cluster_resolver.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">num_accelerators</span><span class="p">(</span>
    <span class="n">task_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">task_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">config_proto</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Returns the number of accelerator cores per worker.</p>
<p>This returns the number of accelerator cores (such as GPUs and TPUs)
available per worker.</p>
<p>Optionally, we allow callers to specify the task_type, and task_id, for
if they want to target a specific TensorFlow process to query
the number of accelerators. This is to support heterogenous environments,
where the number of accelerators cores per host is different.</p>
<h4 id="args_2">Args:</h4>
<ul>
<li><b><code>task_type</code></b>: (Optional) The type of the TensorFlow task of the machine we
  want to query.</li>
<li><b><code>task_id</code></b>: (Optional) The index of the TensorFlow task of the machine we
  want to query.</li>
<li><b><code>config_proto</code></b>: (Optional) Configuration for starting a new session to
  query how many accelerator cores it has.</li>
</ul>
<h4 id="returns_2">Returns:</h4>
<p>A map of accelerator types to number of cores.</p>
    </body>
    </html>
   