<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.distribute.cluster_resolver.UnionResolver" />
<meta itemprop="path" content="Stable" />
<meta itemprop="property" content="environment"/>
<meta itemprop="property" content="rpc_layer"/>
<meta itemprop="property" content="task_id"/>
<meta itemprop="property" content="task_type"/>
<meta itemprop="property" content="__init__"/>
<meta itemprop="property" content="cluster_spec"/>
<meta itemprop="property" content="master"/>
<meta itemprop="property" content="num_accelerators"/>
</div>


<h1>tf.distribute.cluster_resolver.UnionResolver</h1>

<!-- Insert buttons -->




<table class="tfo-notebook-buttons tfo-api" align="left">
</table>


<p><a target="_blank" href="/code/stable/tensorflow/python/distribute/cluster_resolver/cluster_resolver.py">View source</a></p>

<h2>Class <code>UnionResolver</code></h2>

<!-- Start diff -->


<p>Performs a union on underlying ClusterResolvers.</p>

<p>Inherits From: <a href="../../../tf/distribute/cluster_resolver/ClusterResolver.html"><code>ClusterResolver</code></a></p>

<h3>Aliases:</h3>

<ul>
<li>Class <code>tf.compat.v1.distribute.cluster_resolver.UnionResolver</code></li>
<li>Class <code>tf.compat.v2.distribute.cluster_resolver.UnionResolver</code></li>
</ul>


<!-- Placeholder for "Used in" -->


<p>This class performs a union given two or more existing ClusterResolvers. It
merges the underlying ClusterResolvers, and returns one unified ClusterSpec
when cluster_spec is called. The details of the merge function is
documented in the cluster_spec function.</p>

<p>For additional ClusterResolver properties such as task type, task index,
rpc layer, environment, etc&hellip;, we will return the value from the first
ClusterResolver in the union.</p>

<h2 id="__init__"><code>__init__</code></h2>


<p><a target="_blank" href="/code/stable/tensorflow/python/distribute/cluster_resolver/cluster_resolver.py">View source</a></p>

<p><code>python
__init__(
    *args,
    **kwargs
)
</code></p>

<p>Initializes a UnionClusterResolver with other ClusterResolvers.</p>

<h4>Args:</h4>

<ul>
<li><b><code>*args</code></b>: <code>ClusterResolver</code> objects to be unionized.</li>
<li><b><code>**kwargs</code></b>:   rpc_layer - (Optional) Override value for the RPC layer used by
  TensorFlow.
task_type - (Optional) Override value for the current task type.
task_id - (Optional) Override value for the current task index.</li>
</ul>


<h4>Raises:</h4>

<ul>
<li><b><code>TypeError</code></b>: If any argument is not a subclass of <code>ClusterResolvers</code>.</li>
<li><b><code>ValueError</code></b>: If there are no arguments passed.</li>
</ul>


<h2>Properties</h2>

<h3 id="environment"><code>environment</code></h3>


<p>Returns the current environment which TensorFlow is running in.</p>

<p>There are two possible return values, &ldquo;google&rdquo; (when TensorFlow is running
in a Google-internal environment) or an empty string (when TensorFlow is
running elsewhere).</p>

<p>If you are implementing a ClusterResolver that works in both the Google
environment and the open-source world (for instance, a TPU ClusterResolver
or similar), you will have to return the appropriate string depending on the
environment, which you will have to detect.</p>

<p>Otherwise, if you are implementing a ClusterResolver that will only work
in open-source TensorFlow, you do not need to implement this property.</p>

<h3 id="rpc_layer"><code>rpc_layer</code></h3>




<h3 id="task_id"><code>task_id</code></h3>




<h3 id="task_type"><code>task_type</code></h3>


<h2>Methods</h2>

<h3 id="cluster_spec"><code>cluster_spec</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/distribute/cluster_resolver/cluster_resolver.py">View source</a></p>

<p><code>python
cluster_spec()
</code></p>

<p>Returns a union of all the ClusterSpecs from the ClusterResolvers.</p>

<h4>Returns:</h4>

<p>A ClusterSpec containing host information merged from all the underlying
ClusterResolvers.</p>

<h4>Raises:</h4>

<ul>
<li><b><code>KeyError</code></b>: If there are conflicting keys detected when merging two or
more dictionaries, this exception is raised.</li>
</ul>


<p>Note: If there are multiple ClusterResolvers exposing ClusterSpecs with the
same job name, we will merge the list/dict of workers.</p>

<p>If <em>all</em> underlying ClusterSpecs expose the set of workers as lists, we will
concatenate the lists of workers, starting with the list of workers from
the first ClusterResolver passed into the constructor.</p>

<p>If <em>any</em> of the ClusterSpecs expose the set of workers as a dict, we will
treat all the sets of workers as dicts (even if they are returned as lists)
and will only merge them into a dict if there is no conflicting keys. If
there is a conflicting key, we will raise a <code>KeyError</code>.</p>

<h3 id="master"><code>master</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/distribute/cluster_resolver/cluster_resolver.py">View source</a></p>

<p><code>python
master(
    task_type=None,
    task_id=None,
    rpc_layer=None
)
</code></p>

<p>Returns the master address to use when creating a session.</p>

<p>This usually returns the master from the first ClusterResolver passed in,
but you can override this by specifying the task_type and task_id.</p>

<h4>Args:</h4>

<ul>
<li><b><code>task_type</code></b>: (Optional) The type of the TensorFlow task of the master.</li>
<li><b><code>task_id</code></b>: (Optional) The index of the TensorFlow task of the master.</li>
<li><b><code>rpc_layer</code></b>: (Optional) The RPC protocol for the given cluster.</li>
</ul>


<h4>Returns:</h4>

<p>The name or URL of the session master.</p>

<h3 id="num_accelerators"><code>num_accelerators</code></h3>


<p><a target="_blank" href="/code/stable/tensorflow/python/distribute/cluster_resolver/cluster_resolver.py">View source</a></p>

<p><code>python
num_accelerators(
    task_type=None,
    task_id=None,
    config_proto=None
)
</code></p>

<p>Returns the number of accelerator cores per worker.</p>

<p>This returns the number of accelerator cores (such as GPUs and TPUs)
available per worker.</p>

<p>Optionally, we allow callers to specify the task_type, and task_id, for
if they want to target a specific TensorFlow process to query
the number of accelerators. This is to support heterogenous environments,
where the number of accelerators cores per host is different.</p>

<h4>Args:</h4>

<ul>
<li><b><code>task_type</code></b>: (Optional) The type of the TensorFlow task of the machine we
want to query.</li>
<li><b><code>task_id</code></b>: (Optional) The index of the TensorFlow task of the machine we
want to query.</li>
<li><b><code>config_proto</code></b>: (Optional) Configuration for starting a new session to
query how many accelerator cores it has.</li>
</ul>


<h4>Returns:</h4>

<p>A map of accelerator types to number of cores.</p>
