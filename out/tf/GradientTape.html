
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href=../../default.css" rel="stylesheet">
    <link href="
   ../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.GradientTape" />
<meta itemprop="path" content="Stable" />
<meta itemprop="property" content="__enter__"/>
<meta itemprop="property" content="__exit__"/>
<meta itemprop="property" content="__init__"/>
<meta itemprop="property" content="batch_jacobian"/>
<meta itemprop="property" content="gradient"/>
<meta itemprop="property" content="jacobian"/>
<meta itemprop="property" content="reset"/>
<meta itemprop="property" content="stop_recording"/>
<meta itemprop="property" content="watch"/>
<meta itemprop="property" content="watched_variables"/>
</div>

<h1 id="tfgradienttape">tf.GradientTape</h1>
<!-- Insert buttons -->

<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p><a target="_blank" href="/code/stable/tensorflow/python/eager/backprop.py">View source</a></p>
<h2 id="class-gradienttape">Class <code>GradientTape</code></h2>
<!-- Start diff -->

<p>Record operations for automatic differentiation.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.GradientTape</code></li>
<li>Class <code>tf.compat.v2.GradientTape</code></li>
</ul>
<!-- Placeholder for "Used in" -->

<p>Operations are recorded if they are executed within this context manager and
at least one of their inputs is being "watched".</p>
<p>Trainable variables (created by <a href="../tf/Variable.html"><code>tf.Variable</code></a> or <a href="../tf/compat/v1/get_variable.html"><code>tf.compat.v1.get_variable</code></a>,
where <code>trainable=True</code> is default in both cases) are automatically watched.
Tensors can be manually watched by invoking the <code>watch</code> method on this context
manager.</p>
<p>For example, consider the function <code>y = x * x</code>. The gradient at <code>x = 3.0</code> can
be computed as:</p>
<div class="codehilite"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">g</span><span class="p">:</span>
  <span class="n">g</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">dy_dx</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="c1"># Will compute to 6.0</span>
</pre></div>


<p>GradientTapes can be nested to compute higher-order derivatives. For example,</p>
<div class="codehilite"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">g</span><span class="p">:</span>
  <span class="n">g</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">gg</span><span class="p">:</span>
    <span class="n">gg</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
  <span class="n">dy_dx</span> <span class="o">=</span> <span class="n">gg</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>     <span class="c1"># Will compute to 6.0</span>
<span class="n">d2y_dx2</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">dy_dx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># Will compute to 2.0</span>
</pre></div>


<p>By default, the resources held by a GradientTape are released as soon as
GradientTape.gradient() method is called. To compute multiple gradients over
the same computation, create a persistent gradient tape. This allows multiple
calls to the gradient() method as resources are released when the tape object
is garbage collected. For example:</p>
<div class="codehilite"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">g</span><span class="p">:</span>
  <span class="n">g</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span>
<span class="n">dz_dx</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># 108.0 (4*x^3 at x = 3)</span>
<span class="n">dy_dx</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># 6.0</span>
<span class="k">del</span> <span class="n">g</span>  <span class="c1"># Drop the reference to the tape</span>
</pre></div>


<p>By default GradientTape will automatically watch any trainable variables that
are accessed inside the context. If you want fine grained control over which
variables are watched you can disable automatic tracking by passing
<code>watch_accessed_variables=False</code> to the tape constructor:</p>
<div class="codehilite"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">watch_accessed_variables</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
  <span class="n">tape</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">variable_a</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">variable_a</span> <span class="o">**</span> <span class="mi">2</span>  <span class="c1"># Gradients will be available for `variable_a`.</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">variable_b</span> <span class="o">**</span> <span class="mi">3</span>  <span class="c1"># No gradients will be available since `variable_b` is</span>
                       <span class="c1"># not being watched.</span>
</pre></div>


<p>Note that when using models you should ensure that your variables exist when
using <code>watch_accessed_variables=False</code>. Otherwise it's quite easy to make your
first iteration not have any gradients:</p>
<div class="codehilite"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">(</span><span class="n">watch_accessed_variables</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
  <span class="n">tape</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>  <span class="c1"># Since `a.build` has not been called at this point</span>
                           <span class="c1"># `a.variables` will return an empty list and the</span>
                           <span class="c1"># tape will not be watching anything.</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">b</span><span class="p">(</span><span class="n">a</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
  <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>  <span class="c1"># The result of this computation will be</span>
                                      <span class="c1"># a list of `None`s since a&#39;s variables</span>
                                      <span class="c1"># are not being watched.</span>
</pre></div>


<p>Note that only tensors with real or complex dtypes are differentiable.</p>
<h2 id="__init__"><code>__init__</code></h2>

<p><a target="_blank" href="/code/stable/tensorflow/python/eager/backprop.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="fm">__init__</span><span class="p">(</span>
    <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">watch_accessed_variables</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>


<p>Creates a new GradientTape.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>persistent</code></b>: Boolean controlling whether a persistent gradient tape
  is created. False by default, which means at most one call can
  be made to the gradient() method on this object.</li>
<li><b><code>watch_accessed_variables</code></b>: Boolean controlling whether the tape will
  automatically <code>watch</code> any (trainable) variables accessed while the tape
  is active. Defaults to True meaning gradients can be requested from any
  result computed in the tape derived from reading a trainable <code>Variable</code>.
  If False users must explicitly <code>watch</code> any <code>Variable</code>s they want to
  request gradients from.</li>
</ul>
<h2 id="methods">Methods</h2>
<h3 id="__enter__"><code>__enter__</code></h3>

<p><a target="_blank" href="/code/stable/tensorflow/python/eager/backprop.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="fm">__enter__</span><span class="p">()</span>
</pre></div>


<p>Enters a context inside which operations are recorded on this tape.</p>
<h3 id="__exit__"><code>__exit__</code></h3>

<p><a target="_blank" href="/code/stable/tensorflow/python/eager/backprop.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="fm">__exit__</span><span class="p">(</span>
    <span class="n">typ</span><span class="p">,</span>
    <span class="n">value</span><span class="p">,</span>
    <span class="n">traceback</span>
<span class="p">)</span>
</pre></div>


<p>Exits the recording context, no further operations are traced.</p>
<h3 id="batch_jacobian"><code>batch_jacobian</code></h3>

<p><a target="_blank" href="/code/stable/tensorflow/python/eager/backprop.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">batch_jacobian</span><span class="p">(</span>
    <span class="n">target</span><span class="p">,</span>
    <span class="n">source</span><span class="p">,</span>
    <span class="n">unconnected_gradients</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">UnconnectedGradients</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
    <span class="n">parallel_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">experimental_use_pfor</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>


<p>Computes and stacks per-example jacobians.</p>
<p>See <a href="http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant">wikipedia article</a> for the
definition of a Jacobian. This function is essentially an efficient
implementation of the following:</p>
<p><code>tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])</code>.</p>
<p>Note that compared to <a href="../tf/GradientTape.html#jacobian"><code>GradientTape.jacobian</code></a> which computes gradient of
each output value w.r.t each input value, this function is useful when
<code>target[i,...]</code> is independent of <code>source[j,...]</code> for <code>j != i</code>. This
assumption allows more efficient computation as compared to
<a href="../tf/GradientTape.html#jacobian"><code>GradientTape.jacobian</code></a>. The output, as well as intermediate activations,
are lower dimensional and avoid a bunch of redundant zeros which would
result in the jacobian computation given the independence assumption.</p>
<h4 id="example-usage">Example usage:</h4>
<div class="codehilite"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">g</span><span class="p">:</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">g</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">batch_jacobian</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">batch_jacobian</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="c1"># batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]</span>
</pre></div>


<h4 id="args_1">Args:</h4>
<ul>
<li><b><code>target</code></b>: A tensor with rank 2 or higher and with shape [b, y1, ..., y_n].
  <code>target[i,...]</code> should only depend on <code>source[i,...]</code>.</li>
<li><b><code>source</code></b>: A tensor with rank 2 or higher and with shape [b, x1, ..., x_m].</li>
<li><b><code>unconnected_gradients</code></b>: a value which can either hold 'none' or 'zero' and
  alters the value which will be returned if the target and sources are
  unconnected. The possible values and effects are detailed in
  'UnconnectedGradients' and it defaults to 'none'.</li>
<li><b><code>parallel_iterations</code></b>: A knob to control how many iterations are dispatched
  in parallel. This knob can be used to control the total memory usage.</li>
<li><b><code>experimental_use_pfor</code></b>: If true, uses pfor for computing the Jacobian. Else
  uses a tf.while_loop.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>A tensor <code>t</code> with shape [b, y_1, ..., y_n, x1, ..., x_m] where <code>t[i, ...]</code>
is the jacobian of <code>target[i, ...]</code> w.r.t. <code>source[i, ...]</code>, i.e. stacked
per-example jacobians.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>RuntimeError</code></b>: If called on a non-persistent tape with eager execution
  enabled and without enabling experimental_use_pfor.</li>
<li><b><code>ValueError</code></b>: If vectorization of jacobian computation fails or if first
  dimension of <code>target</code> and <code>source</code> do not match.</li>
</ul>
<h3 id="gradient"><code>gradient</code></h3>

<p><a target="_blank" href="/code/stable/tensorflow/python/eager/backprop.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">gradient</span><span class="p">(</span>
    <span class="n">target</span><span class="p">,</span>
    <span class="n">sources</span><span class="p">,</span>
    <span class="n">output_gradients</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">unconnected_gradients</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">UnconnectedGradients</span><span class="o">.</span><span class="n">NONE</span>
<span class="p">)</span>
</pre></div>


<p>Computes the gradient using operations recorded in context of this tape.</p>
<h4 id="args_2">Args:</h4>
<ul>
<li><b><code>target</code></b>: Tensor (or list of tensors) to be differentiated.</li>
<li><b><code>sources</code></b>: a list or nested structure of Tensors or Variables. <code>target</code>
  will be differentiated against elements in <code>sources</code>.</li>
<li><b><code>output_gradients</code></b>: a list of gradients, one for each element of
  target. Defaults to None.</li>
<li><b><code>unconnected_gradients</code></b>: a value which can either hold 'none' or 'zero' and
  alters the value which will be returned if the target and sources are
  unconnected. The possible values and effects are detailed in
  'UnconnectedGradients' and it defaults to 'none'.</li>
</ul>
<h4 id="returns_1">Returns:</h4>
<p>a list or nested structure of Tensors (or IndexedSlices, or None),
one for each element in <code>sources</code>. Returned structure is the same as
the structure of <code>sources</code>.</p>
<h4 id="raises_1">Raises:</h4>
<ul>
<li><b><code>RuntimeError</code></b>: if called inside the context of the tape, or if called more
 than once on a non-persistent tape.</li>
<li><b><code>ValueError</code></b>: if the target is a variable or if unconnected gradients is
 called with an unknown value.</li>
</ul>
<h3 id="jacobian"><code>jacobian</code></h3>

<p><a target="_blank" href="/code/stable/tensorflow/python/eager/backprop.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">jacobian</span><span class="p">(</span>
    <span class="n">target</span><span class="p">,</span>
    <span class="n">sources</span><span class="p">,</span>
    <span class="n">unconnected_gradients</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">UnconnectedGradients</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
    <span class="n">parallel_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">experimental_use_pfor</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>


<p>Computes the jacobian using operations recorded in context of this tape.</p>
<p>See <a href="http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant">wikipedia article</a> for the
definition of a Jacobian.</p>
<h4 id="example-usage_1">Example usage:</h4>
<div class="codehilite"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">g</span><span class="p">:</span>
  <span class="n">x</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
  <span class="n">g</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">jacobian</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">jacobian</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="c1"># jacobian value is [[2., 0.], [0., 4.]]</span>
</pre></div>


<h4 id="args_3">Args:</h4>
<ul>
<li><b><code>target</code></b>: Tensor to be differentiated.</li>
<li><b><code>sources</code></b>: a list or nested structure of Tensors or Variables. <code>target</code>
  will be differentiated against elements in <code>sources</code>.</li>
<li><b><code>unconnected_gradients</code></b>: a value which can either hold 'none' or 'zero' and
  alters the value which will be returned if the target and sources are
  unconnected. The possible values and effects are detailed in
  'UnconnectedGradients' and it defaults to 'none'.</li>
<li><b><code>parallel_iterations</code></b>: A knob to control how many iterations are dispatched
  in parallel. This knob can be used to control the total memory usage.</li>
<li><b><code>experimental_use_pfor</code></b>: If true, vectorizes the jacobian computation. Else
  falls back to a sequential while_loop. Vectorization can sometimes fail
  or lead to excessive memory usage. This option can be used to disable
  vectorization in such cases.</li>
</ul>
<h4 id="returns_2">Returns:</h4>
<p>A list or nested structure of Tensors (or None), one for each element in
<code>sources</code>. Returned structure is the same as the structure of <code>sources</code>.
Note if any gradient is sparse (IndexedSlices), jacobian function
currently makes it dense and returns a Tensor instead. This may change in
the future.</p>
<h4 id="raises_2">Raises:</h4>
<ul>
<li><b><code>RuntimeError</code></b>: If called on a non-persistent tape with eager execution
  enabled and without enabling experimental_use_pfor.</li>
<li><b><code>ValueError</code></b>: If vectorization of jacobian computation fails.</li>
</ul>
<h3 id="reset"><code>reset</code></h3>

<p><a target="_blank" href="/code/stable/tensorflow/python/eager/backprop.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">reset</span><span class="p">()</span>
</pre></div>


<p>Clears all information stored in this tape.</p>
<p>Equivalent to exiting and reentering the tape context manager with a new
tape. For example, the two following code blocks are equivalent:</p>
<div class="codehilite"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
  <span class="n">loss</span> <span class="o">+=</span> <span class="n">other_loss_fn</span><span class="p">()</span>
<span class="n">t</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">...)</span>  <span class="o">#</span> <span class="k">Only</span> <span class="n">differentiates</span> <span class="n">other_loss_fn</span><span class="p">,</span> <span class="k">not</span> <span class="n">loss_fn</span>


<span class="o">#</span> <span class="n">The</span> <span class="n">following</span> <span class="k">is</span> <span class="n">equivalent</span> <span class="k">to</span> <span class="n">the</span> <span class="n">above</span>
<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">()</span>
  <span class="n">t</span><span class="p">.</span><span class="k">reset</span><span class="p">()</span>
  <span class="n">loss</span> <span class="o">+=</span> <span class="n">other_loss_fn</span><span class="p">()</span>
<span class="n">t</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">...)</span>  <span class="o">#</span> <span class="k">Only</span> <span class="n">differentiates</span> <span class="n">other_loss_fn</span><span class="p">,</span> <span class="k">not</span> <span class="n">loss_fn</span>
</pre></div>


<p>This is useful if you don't want to exit the context manager for the tape,
or can't because the desired reset point is inside a control flow construct:</p>
<div class="codehilite"><pre><span></span><span class="err">with tf.GradientTape() as t:</span>
<span class="err">  loss = ...</span>
<span class="err">  if loss &gt; k:</span>
<span class="err">    t.reset()</span>
</pre></div>


<h3 id="stop_recording"><code>stop_recording</code></h3>

<p><a target="_blank" href="/code/stable/tensorflow/python/eager/backprop.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">stop_recording</span><span class="p">()</span>
</pre></div>


<p>Temporarily stops recording operations on this tape.</p>
<p>Operations executed while this context manager is active will not be
recorded on the tape. This is useful for reducing the memory used by tracing
all computations.</p>
<h4 id="for-example">For example:</h4>
<div class="codehilite"><pre><span></span><span class="err">  with tf.GradientTape(persistent=True) as t:</span>
<span class="err">    loss = compute_loss(model)</span>
<span class="err">    with t.stop_recording():</span>
<span class="err">      # The gradient computation below is not traced, saving memory.</span>
<span class="err">      grads = t.gradient(loss, model.variables)</span>
</pre></div>


<h4 id="yields">Yields:</h4>
<p>None</p>
<h4 id="raises_3">Raises:</h4>
<ul>
<li><b><code>RuntimeError</code></b>: if the tape is not currently recording.</li>
</ul>
<h3 id="watch"><code>watch</code></h3>

<p><a target="_blank" href="/code/stable/tensorflow/python/eager/backprop.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">watch</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</pre></div>


<p>Ensures that <code>tensor</code> is being traced by this tape.</p>
<h4 id="args_4">Args:</h4>
<ul>
<li><b><code>tensor</code></b>: a Tensor or list of Tensors.</li>
</ul>
<h4 id="raises_4">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: if it encounters something that is not a tensor.</li>
</ul>
<h3 id="watched_variables"><code>watched_variables</code></h3>

<p><a target="_blank" href="/code/stable/tensorflow/python/eager/backprop.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">watched_variables</span><span class="p">()</span>
</pre></div>


<p>Returns variables watched by this tape in order of construction.</p>
    </body>
    </html>
   