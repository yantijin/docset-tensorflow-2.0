
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href="../../../../default.css" rel="stylesheet">
    <link href="
   ../../../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.keras.losses.BinaryCrossentropy" />
<meta itemprop="path" content="Stable" />
<meta itemprop="property" content="__call__"/>
<meta itemprop="property" content="__init__"/>
<meta itemprop="property" content="from_config"/>
<meta itemprop="property" content="get_config"/>
</div>

<h1 id="tfkeraslossesbinarycrossentropy">tf.keras.losses.BinaryCrossentropy</h1>
<!-- Insert buttons -->

<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/losses.py">View source</a></p>
<h2 id="class-binarycrossentropy">Class <code>BinaryCrossentropy</code></h2>
<!-- Start diff -->

<p>Computes the cross-entropy loss between true labels and predicted labels.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>tf.compat.v1.keras.losses.BinaryCrossentropy</code></li>
<li>Class <code>tf.compat.v2.keras.losses.BinaryCrossentropy</code></li>
<li>Class <code>tf.compat.v2.losses.BinaryCrossentropy</code></li>
<li>Class <code>tf.losses.BinaryCrossentropy</code></li>
</ul>
<!-- Placeholder for "Used in" -->

<p>Use this cross-entropy loss when there are only two label classes (assumed to
be 0 and 1). For each example, there should be a single floating-point value
per prediction.</p>
<p>In the snippet below, each of the four examples has only a single
floating-pointing value, and both <code>y_pred</code> and <code>y_true</code> have the shape
<code>[batch_size]</code>.</p>
<h4 id="usage">Usage:</h4>
<div class="codehilite"><pre><span></span><span class="n">bce</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">bce</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loss: &#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># Loss: 11.522857</span>
</pre></div>


<p>Usage with the <a href="../../../tf/keras.html"><code>tf.keras</code></a> API:</p>
<div class="codehilite"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;sgd&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">())</span>
</pre></div>


<h4 id="args">Args:</h4>
<ul>
<li><b><code>from_logits</code></b>: Whether to interpret <code>y_pred</code> as a tensor of
  <a href="https://en.wikipedia.org/wiki/Logit">logit</a> values. By default, we assume
    that <code>y_pred</code> contains probabilities (i.e., values in [0, 1]).
  Note: Using from_logits=True may be more numerically stable.</li>
<li><b><code>label_smoothing</code></b>: Float in [0, 1]. When 0, no smoothing occurs. When &gt; 0, we
  compute the loss between the predicted labels and a smoothed version of
  the true labels, where the smoothing squeezes the labels towards 0.5.
  Larger values of <code>label_smoothing</code> correspond to heavier smoothing.</li>
<li><b><code>reduction</code></b>: (Optional) Type of <a href="../../../tf/keras/losses/Reduction.html"><code>tf.keras.losses.Reduction</code></a> to apply to loss.
  Default value is <code>AUTO</code>. <code>AUTO</code> indicates that the reduction option will
  be determined by the usage context. For almost all cases this defaults to
  <code>SUM_OVER_BATCH_SIZE</code>.
  When used with <a href="../../../tf/distribute/Strategy.html"><code>tf.distribute.Strategy</code></a>, outside of built-in training
  loops such as <a href="../../../tf/keras.html"><code>tf.keras</code></a> <code>compile</code> and <code>fit</code>, using <code>AUTO</code> or
  <code>SUM_OVER_BATCH_SIZE</code> will raise an error. Please see
  https://www.tensorflow.org/alpha/tutorials/distribute/training_loops
  for more details on this.</li>
<li><b><code>name</code></b>: (Optional) Name for the op.</li>
</ul>
<h2 id="__init__"><code>__init__</code></h2>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/losses.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="fm">__init__</span><span class="p">(</span>
    <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">label_smoothing</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="n">losses_utils</span><span class="o">.</span><span class="n">ReductionV2</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span>
<span class="p">)</span>
</pre></div>


<p>Initialize self.  See help(type(self)) for accurate signature.</p>
<h2 id="methods">Methods</h2>
<h3 id="__call__"><code>__call__</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/losses.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="fm">__call__</span><span class="p">(</span>
    <span class="n">y_true</span><span class="p">,</span>
    <span class="n">y_pred</span><span class="p">,</span>
    <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<p>Invokes the <code>Loss</code> instance.</p>
<h4 id="args_1">Args:</h4>
<ul>
<li><b><code>y_true</code></b>: Ground truth values. shape = <code>[batch_size, d0, .. dN]</code></li>
<li><b><code>y_pred</code></b>: The predicted values. shape = <code>[batch_size, d0, .. dN]</code></li>
<li><b><code>sample_weight</code></b>: Optional <code>sample_weight</code> acts as a
  coefficient for the loss. If a scalar is provided, then the loss is
  simply scaled by the given value. If <code>sample_weight</code> is a tensor of size
  <code>[batch_size]</code>, then the total loss for each sample of the batch is
  rescaled by the corresponding element in the <code>sample_weight</code> vector. If
  the shape of <code>sample_weight</code> is <code>[batch_size, d0, .. dN-1]</code> (or can be
  broadcasted to this shape), then each loss element of <code>y_pred</code> is scaled
  by the corresponding value of <code>sample_weight</code>. (Note on<code>dN-1</code>: all loss
  functions reduce by 1 dimension, usually axis=-1.)</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>Weighted loss float <code>Tensor</code>. If <code>reduction</code> is <code>NONE</code>, this has
  shape <code>[batch_size, d0, .. dN-1]</code>; otherwise, it is scalar. (Note <code>dN-1</code>
  because all loss functions reduce by 1 dimension, usually axis=-1.)</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: If the shape of <code>sample_weight</code> is invalid.</li>
</ul>
<h3 id="from_config"><code>from_config</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/losses.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">from_config</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span>
    <span class="n">config</span>
<span class="p">)</span>
</pre></div>


<p>Instantiates a <code>Loss</code> from its config (output of <code>get_config()</code>).</p>
<h4 id="args_2">Args:</h4>
<ul>
<li><b><code>config</code></b>: Output of <code>get_config()</code>.</li>
</ul>
<h4 id="returns_1">Returns:</h4>
<p>A <code>Loss</code> instance.</p>
<h3 id="get_config"><code>get_config</code></h3>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/losses.py">View source</a></p>
<div class="codehilite"><pre><span></span><span class="n">get_config</span><span class="p">()</span>
</pre></div>
    </body>
    </html>
   