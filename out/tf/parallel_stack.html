
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href="../../default.css" rel="stylesheet">
    <link href="
   ../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.parallel_stack" />
<meta itemprop="path" content="Stable" />
</div>

<h1 id="tfparallel_stack">tf.parallel_stack</h1>
<!-- Insert buttons -->

<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/array_ops.py">View source</a></p>
<!-- Start diff -->

<p>Stacks a list of rank-<code>R</code> tensors into one rank-<code>(R+1)</code> tensor in parallel.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v1.parallel_stack</code></li>
<li><code>tf.compat.v2.parallel_stack</code></li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">parallel_stack</span><span class="p">(</span>
    <span class="n">values</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;parallel_stack&#39;</span>
<span class="p">)</span>
</pre></div>


<!-- Placeholder for "Used in" -->

<p>Requires that the shape of inputs be known at graph construction time.</p>
<p>Packs the list of tensors in <code>values</code> into a tensor with rank one higher than
each tensor in <code>values</code>, by packing them along the first dimension.
Given a list of length <code>N</code> of tensors of shape <code>(A, B, C)</code>; the <code>output</code>
tensor will have the shape <code>(N, A, B, C)</code>.</p>
<h4 id="for-example">For example:</h4>
<div class="codehilite"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">tf</span><span class="o">.</span><span class="n">parallel_stack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">])</span>  <span class="c1"># [[1, 4], [2, 5], [3, 6]]</span>
</pre></div>


<p>The difference between <code>stack</code> and <code>parallel_stack</code> is that <code>stack</code> requires
all the inputs be computed before the operation will begin but doesn't require
that the input shapes be known during graph construction.</p>
<p><code>parallel_stack</code> will copy pieces of the input into the output as they become
available, in some situations this can provide a performance benefit.</p>
<p>Unlike <code>stack</code>, <code>parallel_stack</code> does NOT support backpropagation.</p>
<p>This is the opposite of unstack.  The numpy equivalent is</p>
<div class="codehilite"><pre><span></span><span class="err">tf.parallel_stack([x, y, z]) = np.asarray([x, y, z])</span>
</pre></div>


<h4 id="args">Args:</h4>
<ul>
<li><b><code>values</code></b>: A list of <code>Tensor</code> objects with the same shape and type.</li>
<li><b><code>name</code></b>: A name for this operation (optional).</li>
</ul>
<h4 id="returns">Returns:</h4>
<ul>
<li><b><code>output</code></b>: A stacked <code>Tensor</code> with the same type as <code>values</code>.</li>
</ul>
    </body>
    </html>
   