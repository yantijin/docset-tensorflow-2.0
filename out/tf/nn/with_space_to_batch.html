
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href="../../../default.css" rel="stylesheet">
    <link href="
   ../../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.nn.with_space_to_batch" />
<meta itemprop="path" content="Stable" />
</div>

<h1 id="tfnnwith_space_to_batch">tf.nn.with_space_to_batch</h1>
<!-- Insert buttons -->

<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/nn_ops.py">View source</a></p>
<!-- Start diff -->

<p>Performs <code>op</code> on the space-to-batch representation of <code>input</code>.</p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li><code>tf.compat.v1.nn.with_space_to_batch</code></li>
<li><code>tf.compat.v2.nn.with_space_to_batch</code></li>
</ul>
<div class="codehilite"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">with_space_to_batch</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">,</span>
    <span class="n">dilation_rate</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">,</span>
    <span class="n">op</span><span class="p">,</span>
    <span class="n">filter_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">spatial_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">data_format</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>


<!-- Placeholder for "Used in" -->

<p>This has the effect of transforming sliding window operations into the
corresponding "atrous" operation in which the input is sampled at the
specified <code>dilation_rate</code>.</p>
<p>In the special case that <code>dilation_rate</code> is uniformly 1, this simply returns:</p>
<p>op(input, num_spatial_dims, padding)</p>
<p>Otherwise, it returns:</p>
<p>batch_to_space_nd(
    op(space_to_batch_nd(input, adjusted_dilation_rate, adjusted_paddings),
       num_spatial_dims,
       "VALID")
    adjusted_dilation_rate,
    adjusted_crops),</p>
<p>where:</p>
<p>adjusted_dilation_rate is an int64 tensor of shape [max(spatial_dims)],
  adjusted_{paddings,crops} are int64 tensors of shape [max(spatial_dims), 2]</p>
<p>defined as follows:</p>
<p>We first define two int64 tensors <code>paddings</code> and <code>crops</code> of shape
<code>[num_spatial_dims, 2]</code> based on the value of <code>padding</code> and the spatial
dimensions of the <code>input</code>:</p>
<p>If <code>padding = "VALID"</code>, then:</p>
<p>paddings, crops = required_space_to_batch_paddings(
    input_shape[spatial_dims],
    dilation_rate)</p>
<p>If <code>padding = "SAME"</code>, then:</p>
<p>dilated_filter_shape =
    filter_shape + (filter_shape - 1) * (dilation_rate - 1)</p>
<p>paddings, crops = required_space_to_batch_paddings(
    input_shape[spatial_dims],
    dilation_rate,
    [(dilated_filter_shape - 1) // 2,
     dilated_filter_shape - 1 - (dilated_filter_shape - 1) // 2])</p>
<p>Because <code>space_to_batch_nd</code> and <code>batch_to_space_nd</code> assume that the spatial
dimensions are contiguous starting at the second dimension, but the specified
<code>spatial_dims</code> may not be, we must adjust <code>dilation_rate</code>, <code>paddings</code> and
<code>crops</code> in order to be usable with these operations.  For a given dimension,
if the block size is 1, and both the starting and ending padding and crop
amounts are 0, then space_to_batch_nd effectively leaves that dimension alone,
which is what is needed for dimensions not part of <code>spatial_dims</code>.
Furthermore, <code>space_to_batch_nd</code> and <code>batch_to_space_nd</code> handle this case
efficiently for any number of leading and trailing dimensions.</p>
<p>For 0 &lt;= i &lt; len(spatial_dims), we assign:</p>
<p>adjusted_dilation_rate[spatial_dims[i] - 1] = dilation_rate[i]
  adjusted_paddings[spatial_dims[i] - 1, :] = paddings[i, :]
  adjusted_crops[spatial_dims[i] - 1, :] = crops[i, :]</p>
<p>All unassigned values of <code>adjusted_dilation_rate</code> default to 1, while all
unassigned values of <code>adjusted_paddings</code> and <code>adjusted_crops</code> default to 0.</p>
<p>Note in the case that <code>dilation_rate</code> is not uniformly 1, specifying "VALID"
padding is equivalent to specifying <code>padding = "SAME"</code> with a filter_shape of
<code>[1]*N</code>.</p>
<p>Advanced usage. Note the following optimization: A sequence of
<code>with_space_to_batch</code> operations with identical (not uniformly 1)
<code>dilation_rate</code> parameters and "VALID" padding</p>
<p>net = with_space_to_batch(net, dilation_rate, "VALID", op_1)
  ...
  net = with_space_to_batch(net, dilation_rate, "VALID", op_k)</p>
<p>can be combined into a single <code>with_space_to_batch</code> operation as follows:</p>
<p>def combined_op(converted_input, num_spatial_dims, _):
    result = op_1(converted_input, num_spatial_dims, "VALID")
    ...
    result = op_k(result, num_spatial_dims, "VALID")</p>
<p>net = with_space_to_batch(net, dilation_rate, "VALID", combined_op)</p>
<p>This eliminates the overhead of <code>k-1</code> calls to <code>space_to_batch_nd</code> and
<code>batch_to_space_nd</code>.</p>
<p>Similarly, a sequence of <code>with_space_to_batch</code> operations with identical (not
uniformly 1) <code>dilation_rate</code> parameters, "SAME" padding, and odd filter
dimensions</p>
<p>net = with_space_to_batch(net, dilation_rate, "SAME", op_1, filter_shape_1)
  ...
  net = with_space_to_batch(net, dilation_rate, "SAME", op_k, filter_shape_k)</p>
<p>can be combined into a single <code>with_space_to_batch</code> operation as follows:</p>
<p>def combined_op(converted_input, num_spatial_dims, _):
    result = op_1(converted_input, num_spatial_dims, "SAME")
    ...
    result = op_k(result, num_spatial_dims, "SAME")</p>
<p>net = with_space_to_batch(net, dilation_rate, "VALID", combined_op)</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>input</code></b>: Tensor of rank &gt; max(spatial_dims).</li>
<li><b><code>dilation_rate</code></b>: int32 Tensor of <em>known</em> shape [num_spatial_dims].</li>
<li><b><code>padding</code></b>: str constant equal to "VALID" or "SAME"</li>
<li><b><code>op</code></b>: Function that maps (input, num_spatial_dims, padding) -&gt; output</li>
<li><b><code>filter_shape</code></b>: If padding = "SAME", specifies the shape of the convolution
  kernel/pooling window as an integer Tensor of shape [&gt;=num_spatial_dims].
  If padding = "VALID", filter_shape is ignored and need not be specified.</li>
<li><b><code>spatial_dims</code></b>: Monotonically increasing sequence of <code>num_spatial_dims</code>
  integers (which are &gt;= 1) specifying the spatial dimensions of <code>input</code>
  and output.  Defaults to: <code>range(1, num_spatial_dims+1)</code>.</li>
<li><b><code>data_format</code></b>: A string or None.  Specifies whether the channel dimension of
  the <code>input</code> and output is the last dimension (default, or if <code>data_format</code>
  does not start with "NC"), or the second dimension (if <code>data_format</code>
  starts with "NC").  For N=1, the valid values are "NWC" (default) and
  "NCW".  For N=2, the valid values are "NHWC" (default) and "NCHW".
  For N=3, the valid values are "NDHWC" (default) and "NCDHW".</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>The output Tensor as described above, dimensions will vary based on the op
provided.</p>
<h4 id="raises">Raises:</h4>
<ul>
<li><b><code>ValueError</code></b>: if <code>padding</code> is invalid or the arguments are incompatible.</li>
<li><b><code>ValueError</code></b>: if <code>spatial_dims</code> are invalid.</li>
</ul>
    </body>
    </html>
   