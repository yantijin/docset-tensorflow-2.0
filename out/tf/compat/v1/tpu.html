
    <html lang="zh-cn">
    <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <link href="../../../../default.css" rel="stylesheet">
    <link href="
   ../../../../github.css" rel="stylesheet">
    </head>
    <body>
    <div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tf.compat.v1.tpu" />
<meta itemprop="path" content="Stable" />
</div>

<h1 id="module-tfcompatv1tpu">Module: tf.compat.v1.tpu</h1>
<table class="tfo-notebook-buttons tfo-api" align="left">
</table>

<p>Ops related to Tensor Processing Units.</p>
<h2 id="modules">Modules</h2>
<p><a href="../../../tf/compat/v1/tpu/experimental.html"><code>experimental</code></a> module: Public API for tf.tpu.experimental namespace.</p>
<h2 id="classes">Classes</h2>
<p><a href="../../../tf/compat/v1/tpu/CrossShardOptimizer.html"><code>class CrossShardOptimizer</code></a>: An optimizer that averages gradients across TPU shards.</p>
<h2 id="functions">Functions</h2>
<p><a href="../../../tf/compat/v1/tpu/batch_parallel.html"><code>batch_parallel(...)</code></a>: Shards <code>computation</code> along the batch dimension for parallel execution.</p>
<p><a href="../../../tf/compat/v1/tpu/bfloat16_scope.html"><code>bfloat16_scope(...)</code></a>: Scope class for bfloat16 variables so that the model uses custom getter.</p>
<p><a href="../../../tf/compat/v1/tpu/core.html"><code>core(...)</code></a>: Returns the device name for a core in a replicated TPU computation.</p>
<p><a href="../../../tf/compat/v1/tpu/cross_replica_sum.html"><code>cross_replica_sum(...)</code></a>: Sum the input tensor across replicas according to group_assignment.</p>
<p><a href="../../../tf/compat/v1/tpu/initialize_system.html"><code>initialize_system(...)</code></a>: Initializes a distributed TPU system for use with TensorFlow.</p>
<p><a href="../../../tf/compat/v1/tpu/outside_compilation.html"><code>outside_compilation(...)</code></a>: Builds part of a computation outside any current TPU replicate scope.</p>
<p><a href="../../../tf/compat/v1/tpu/replicate.html"><code>replicate(...)</code></a>: Builds a graph operator that runs a replicated TPU computation.</p>
<p><a href="../../../tf/compat/v1/tpu/rewrite.html"><code>rewrite(...)</code></a>: Rewrites <code>computation</code> for execution on a TPU system.</p>
<p><a href="../../../tf/compat/v1/tpu/shard.html"><code>shard(...)</code></a>: Shards <code>computation</code> for parallel execution.</p>
<p><a href="../../../tf/compat/v1/tpu/shutdown_system.html"><code>shutdown_system(...)</code></a>: Shuts down a running a distributed TPU system.</p>
    </body>
    </html>
   